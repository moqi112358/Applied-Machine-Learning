2017-05-11 07:05:33.560385: step 0, loss = 4.68, accuracy = 0.08 (2.4 examples/sec; 52.774 sec/batch)
2017-05-11 07:06:13.790613: step 100, loss = 4.12, accuracy = 0.32 (314.7 examples/sec; 0.407 sec/batch)
2017-05-11 07:06:51.471671: step 200, loss = 3.75, accuracy = 0.37 (527.0 examples/sec; 0.243 sec/batch)
2017-05-11 07:07:28.752485: step 300, loss = 3.53, accuracy = 0.44 (388.3 examples/sec; 0.330 sec/batch)
2017-05-11 07:08:06.381118: step 400, loss = 3.38, accuracy = 0.37 (352.8 examples/sec; 0.363 sec/batch)
2017-05-11 07:08:43.954916: step 500, loss = 3.16, accuracy = 0.44 (275.6 examples/sec; 0.464 sec/batch)
2017-05-11 07:09:21.618457: step 600, loss = 2.87, accuracy = 0.52 (335.8 examples/sec; 0.381 sec/batch)
2017-05-11 07:09:56.083243: step 700, loss = 2.95, accuracy = 0.41 (348.1 examples/sec; 0.368 sec/batch)
2017-05-11 07:10:32.598987: step 800, loss = 2.87, accuracy = 0.41 (295.8 examples/sec; 0.433 sec/batch)
2017-05-11 07:11:09.542634: step 900, loss = 2.64, accuracy = 0.48 (344.4 examples/sec; 0.372 sec/batch)
2017-05-11 07:11:48.085281: step 1000, loss = 2.49, accuracy = 0.51 (322.7 examples/sec; 0.397 sec/batch)
2017-05-11 07:12:26.360101: step 1100, loss = 2.32, accuracy = 0.56 (416.1 examples/sec; 0.308 sec/batch)
2017-05-11 07:13:01.775815: step 1200, loss = 2.01, accuracy = 0.62 (332.6 examples/sec; 0.385 sec/batch)
2017-05-11 07:13:38.365448: step 1300, loss = 2.08, accuracy = 0.61 (361.5 examples/sec; 0.354 sec/batch)
2017-05-11 07:14:15.409567: step 1400, loss = 2.03, accuracy = 0.55 (359.7 examples/sec; 0.356 sec/batch)
2017-05-11 07:14:53.295263: step 1500, loss = 2.22, accuracy = 0.52 (321.3 examples/sec; 0.398 sec/batch)
2017-05-11 07:15:30.023050: step 1600, loss = 1.95, accuracy = 0.55 (278.5 examples/sec; 0.460 sec/batch)
2017-05-11 07:16:07.233857: step 1700, loss = 1.79, accuracy = 0.57 (417.9 examples/sec; 0.306 sec/batch)
2017-05-11 07:16:45.549460: step 1800, loss = 1.68, accuracy = 0.62 (365.3 examples/sec; 0.350 sec/batch)
2017-05-11 07:17:23.001325: step 1900, loss = 1.71, accuracy = 0.63 (358.8 examples/sec; 0.357 sec/batch)
2017-05-11 07:17:58.783034: step 2000, loss = 1.67, accuracy = 0.60 (561.9 examples/sec; 0.228 sec/batch)
2017-05-11 07:18:35.311765: step 2100, loss = 1.60, accuracy = 0.65 (362.3 examples/sec; 0.353 sec/batch)
2017-05-11 07:19:11.119744: step 2200, loss = 1.73, accuracy = 0.59 (320.3 examples/sec; 0.400 sec/batch)
2017-05-11 07:19:48.579707: step 2300, loss = 1.48, accuracy = 0.63 (341.2 examples/sec; 0.375 sec/batch)
2017-05-11 07:20:23.133160: step 2400, loss = 1.44, accuracy = 0.66 (352.6 examples/sec; 0.363 sec/batch)
2017-05-11 07:21:01.137898: step 2500, loss = 1.53, accuracy = 0.58 (264.4 examples/sec; 0.484 sec/batch)
2017-05-11 07:21:38.019227: step 2600, loss = 1.37, accuracy = 0.71 (368.1 examples/sec; 0.348 sec/batch)
2017-05-11 07:22:14.820012: step 2700, loss = 1.49, accuracy = 0.59 (564.0 examples/sec; 0.227 sec/batch)
2017-05-11 07:22:51.779059: step 2800, loss = 1.34, accuracy = 0.67 (397.6 examples/sec; 0.322 sec/batch)
2017-05-11 07:23:29.010548: step 2900, loss = 1.22, accuracy = 0.72 (350.7 examples/sec; 0.365 sec/batch)
2017-05-11 07:24:07.089911: step 3000, loss = 1.24, accuracy = 0.70 (340.9 examples/sec; 0.376 sec/batch)
2017-05-11 07:24:44.896884: step 3100, loss = 1.20, accuracy = 0.65 (378.2 examples/sec; 0.338 sec/batch)
2017-05-11 07:25:22.177234: step 3200, loss = 1.26, accuracy = 0.63 (340.7 examples/sec; 0.376 sec/batch)
2017-05-11 07:25:58.530998: step 3300, loss = 1.33, accuracy = 0.68 (336.5 examples/sec; 0.380 sec/batch)
2017-05-11 07:26:36.293331: step 3400, loss = 1.29, accuracy = 0.66 (591.4 examples/sec; 0.216 sec/batch)
2017-05-11 07:27:11.361966: step 3500, loss = 1.23, accuracy = 0.73 (352.6 examples/sec; 0.363 sec/batch)
2017-05-11 07:27:47.905966: step 3600, loss = 1.01, accuracy = 0.77 (381.2 examples/sec; 0.336 sec/batch)
2017-05-11 07:28:26.826439: step 3700, loss = 1.10, accuracy = 0.70 (321.0 examples/sec; 0.399 sec/batch)
2017-05-11 07:29:08.571054: step 3800, loss = 1.09, accuracy = 0.71 (313.1 examples/sec; 0.409 sec/batch)
2017-05-11 07:29:47.165023: step 3900, loss = 1.23, accuracy = 0.69 (353.5 examples/sec; 0.362 sec/batch)
2017-05-11 07:30:31.432677: step 4000, loss = 1.09, accuracy = 0.68 (250.7 examples/sec; 0.511 sec/batch)
2017-05-11 07:31:14.280740: step 4100, loss = 1.11, accuracy = 0.68 (324.6 examples/sec; 0.394 sec/batch)
2017-05-11 07:31:54.941792: step 4200, loss = 1.01, accuracy = 0.69 (513.6 examples/sec; 0.249 sec/batch)
2017-05-11 07:32:39.339990: step 4300, loss = 1.04, accuracy = 0.70 (399.3 examples/sec; 0.321 sec/batch)
2017-05-11 07:33:21.087635: step 4400, loss = 1.04, accuracy = 0.73 (255.6 examples/sec; 0.501 sec/batch)
2017-05-11 07:34:04.346489: step 4500, loss = 1.09, accuracy = 0.71 (324.4 examples/sec; 0.395 sec/batch)
2017-05-11 07:34:45.910733: step 4600, loss = 1.12, accuracy = 0.69 (390.5 examples/sec; 0.328 sec/batch)
2017-05-11 07:35:28.512941: step 4700, loss = 0.95, accuracy = 0.77 (318.9 examples/sec; 0.401 sec/batch)
2017-05-11 07:36:12.427520: step 4800, loss = 0.98, accuracy = 0.71 (206.9 examples/sec; 0.619 sec/batch)
2017-05-11 07:36:54.569118: step 4900, loss = 0.93, accuracy = 0.79 (402.8 examples/sec; 0.318 sec/batch)
2017-05-11 07:37:35.916493: step 5000, loss = 0.89, accuracy = 0.77 (282.7 examples/sec; 0.453 sec/batch)
2017-05-11 07:38:14.484073: step 5100, loss = 0.92, accuracy = 0.73 (312.8 examples/sec; 0.409 sec/batch)
2017-05-11 07:38:50.391069: step 5200, loss = 0.92, accuracy = 0.80 (360.4 examples/sec; 0.355 sec/batch)
2017-05-11 07:39:28.257524: step 5300, loss = 0.83, accuracy = 0.79 (326.9 examples/sec; 0.392 sec/batch)
2017-05-11 07:40:06.221877: step 5400, loss = 1.17, accuracy = 0.67 (297.2 examples/sec; 0.431 sec/batch)
2017-05-11 07:40:43.027071: step 5500, loss = 1.08, accuracy = 0.70 (338.5 examples/sec; 0.378 sec/batch)
2017-05-11 07:41:18.867097: step 5600, loss = 1.06, accuracy = 0.70 (326.9 examples/sec; 0.392 sec/batch)
2017-05-11 07:41:55.495451: step 5700, loss = 1.03, accuracy = 0.68 (306.0 examples/sec; 0.418 sec/batch)
2017-05-11 07:42:33.150553: step 5800, loss = 1.01, accuracy = 0.74 (312.9 examples/sec; 0.409 sec/batch)
2017-05-11 07:43:10.465007: step 5900, loss = 0.80, accuracy = 0.77 (325.9 examples/sec; 0.393 sec/batch)
2017-05-11 07:43:47.213294: step 6000, loss = 1.13, accuracy = 0.62 (320.8 examples/sec; 0.399 sec/batch)
2017-05-11 07:44:23.240642: step 6100, loss = 0.86, accuracy = 0.80 (390.7 examples/sec; 0.328 sec/batch)
2017-05-11 07:45:01.654775: step 6200, loss = 0.93, accuracy = 0.77 (290.0 examples/sec; 0.441 sec/batch)
2017-05-11 07:45:36.281958: step 6300, loss = 0.81, accuracy = 0.77 (451.6 examples/sec; 0.283 sec/batch)
2017-05-11 07:46:13.660683: step 6400, loss = 1.01, accuracy = 0.67 (315.9 examples/sec; 0.405 sec/batch)
2017-05-11 07:46:50.297742: step 6500, loss = 0.99, accuracy = 0.70 (333.7 examples/sec; 0.384 sec/batch)
2017-05-11 07:47:27.083013: step 6600, loss = 0.95, accuracy = 0.70 (391.6 examples/sec; 0.327 sec/batch)
2017-05-11 07:48:04.889865: step 6700, loss = 0.85, accuracy = 0.74 (361.5 examples/sec; 0.354 sec/batch)
2017-05-11 07:48:40.240671: step 6800, loss = 0.84, accuracy = 0.77 (339.8 examples/sec; 0.377 sec/batch)
2017-05-11 07:49:18.035934: step 6900, loss = 0.98, accuracy = 0.73 (360.5 examples/sec; 0.355 sec/batch)
2017-05-11 07:49:53.767968: step 7000, loss = 0.95, accuracy = 0.70 (366.9 examples/sec; 0.349 sec/batch)
2017-05-11 07:50:32.336285: step 7100, loss = 0.87, accuracy = 0.77 (325.8 examples/sec; 0.393 sec/batch)
2017-05-11 07:51:09.923714: step 7200, loss = 1.15, accuracy = 0.66 (433.2 examples/sec; 0.295 sec/batch)
2017-05-11 07:51:46.467086: step 7300, loss = 0.83, accuracy = 0.84 (376.5 examples/sec; 0.340 sec/batch)
2017-05-11 07:52:23.158160: step 7400, loss = 1.15, accuracy = 0.65 (311.0 examples/sec; 0.412 sec/batch)
2017-05-11 07:52:59.725440: step 7500, loss = 0.81, accuracy = 0.82 (361.6 examples/sec; 0.354 sec/batch)
2017-05-11 07:53:39.009657: step 7600, loss = 0.99, accuracy = 0.73 (264.8 examples/sec; 0.483 sec/batch)
2017-05-11 07:54:15.823872: step 7700, loss = 1.02, accuracy = 0.77 (292.3 examples/sec; 0.438 sec/batch)
2017-05-11 07:54:54.483463: step 7800, loss = 0.89, accuracy = 0.73 (306.6 examples/sec; 0.418 sec/batch)
2017-05-11 07:55:31.794570: step 7900, loss = 0.93, accuracy = 0.77 (358.7 examples/sec; 0.357 sec/batch)
2017-05-11 07:56:10.167951: step 8000, loss = 1.07, accuracy = 0.67 (367.2 examples/sec; 0.349 sec/batch)
2017-05-11 07:56:47.829504: step 8100, loss = 0.86, accuracy = 0.77 (304.6 examples/sec; 0.420 sec/batch)
2017-05-11 07:57:26.933416: step 8200, loss = 0.94, accuracy = 0.69 (348.9 examples/sec; 0.367 sec/batch)
2017-05-11 07:58:03.047003: step 8300, loss = 0.95, accuracy = 0.70 (565.8 examples/sec; 0.226 sec/batch)
2017-05-11 07:58:41.083300: step 8400, loss = 0.97, accuracy = 0.73 (310.8 examples/sec; 0.412 sec/batch)
2017-05-11 07:59:16.168887: step 8500, loss = 0.95, accuracy = 0.75 (470.9 examples/sec; 0.272 sec/batch)
2017-05-11 07:59:54.603690: step 8600, loss = 0.70, accuracy = 0.82 (313.0 examples/sec; 0.409 sec/batch)
2017-05-11 08:00:31.994249: step 8700, loss = 0.96, accuracy = 0.73 (332.3 examples/sec; 0.385 sec/batch)
2017-05-11 08:01:08.792127: step 8800, loss = 0.81, accuracy = 0.74 (319.3 examples/sec; 0.401 sec/batch)
2017-05-11 08:01:46.771174: step 8900, loss = 0.98, accuracy = 0.71 (354.5 examples/sec; 0.361 sec/batch)
2017-05-11 08:02:24.174106: step 9000, loss = 0.99, accuracy = 0.72 (338.7 examples/sec; 0.378 sec/batch)
2017-05-11 08:03:01.261529: step 9100, loss = 0.77, accuracy = 0.82 (371.8 examples/sec; 0.344 sec/batch)
2017-05-11 08:03:39.885180: step 9200, loss = 1.07, accuracy = 0.67 (340.4 examples/sec; 0.376 sec/batch)
2017-05-11 08:04:16.350887: step 9300, loss = 0.87, accuracy = 0.77 (522.4 examples/sec; 0.245 sec/batch)
2017-05-11 08:04:53.676032: step 9400, loss = 0.98, accuracy = 0.74 (337.7 examples/sec; 0.379 sec/batch)
2017-05-11 08:05:31.161115: step 9500, loss = 0.88, accuracy = 0.77 (282.4 examples/sec; 0.453 sec/batch)
2017-05-11 08:06:07.218458: step 9600, loss = 1.04, accuracy = 0.71 (327.6 examples/sec; 0.391 sec/batch)
2017-05-11 08:06:45.446558: step 9700, loss = 0.80, accuracy = 0.74 (371.8 examples/sec; 0.344 sec/batch)
2017-05-11 08:07:22.915033: step 9800, loss = 0.97, accuracy = 0.71 (313.2 examples/sec; 0.409 sec/batch)
2017-05-11 08:08:01.402498: step 9900, loss = 0.88, accuracy = 0.77 (318.8 examples/sec; 0.402 sec/batch)
2017-05-11 08:08:38.175391: step 10000, loss = 0.82, accuracy = 0.77 (393.7 examples/sec; 0.325 sec/batch)
2017-05-11 08:09:15.382963: step 10100, loss = 0.96, accuracy = 0.74 (356.4 examples/sec; 0.359 sec/batch)
2017-05-11 08:09:53.166275: step 10200, loss = 0.73, accuracy = 0.80 (355.4 examples/sec; 0.360 sec/batch)
2017-05-11 08:10:29.766700: step 10300, loss = 0.98, accuracy = 0.73 (342.7 examples/sec; 0.373 sec/batch)
2017-05-11 08:11:07.605987: step 10400, loss = 0.84, accuracy = 0.77 (373.1 examples/sec; 0.343 sec/batch)
2017-05-11 08:11:43.313344: step 10500, loss = 0.79, accuracy = 0.83 (346.7 examples/sec; 0.369 sec/batch)
2017-05-11 08:12:20.353973: step 10600, loss = 1.04, accuracy = 0.77 (469.3 examples/sec; 0.273 sec/batch)
2017-05-11 08:12:57.505131: step 10700, loss = 0.83, accuracy = 0.80 (490.9 examples/sec; 0.261 sec/batch)
2017-05-11 08:13:36.333886: step 10800, loss = 0.70, accuracy = 0.83 (488.3 examples/sec; 0.262 sec/batch)
2017-05-11 08:14:14.487263: step 10900, loss = 0.90, accuracy = 0.79 (379.5 examples/sec; 0.337 sec/batch)
2017-05-11 08:14:53.450738: step 11000, loss = 0.93, accuracy = 0.76 (334.5 examples/sec; 0.383 sec/batch)
2017-05-11 08:15:30.527367: step 11100, loss = 0.80, accuracy = 0.77 (454.9 examples/sec; 0.281 sec/batch)
2017-05-11 08:16:07.199777: step 11200, loss = 0.80, accuracy = 0.79 (344.2 examples/sec; 0.372 sec/batch)
2017-05-11 08:16:44.595870: step 11300, loss = 0.96, accuracy = 0.71 (287.4 examples/sec; 0.445 sec/batch)
2017-05-11 08:17:20.155070: step 11400, loss = 0.91, accuracy = 0.76 (350.1 examples/sec; 0.366 sec/batch)
2017-05-11 08:17:57.809902: step 11500, loss = 0.78, accuracy = 0.80 (467.8 examples/sec; 0.274 sec/batch)
2017-05-11 08:18:34.464845: step 11600, loss = 0.92, accuracy = 0.74 (362.4 examples/sec; 0.353 sec/batch)
2017-05-11 08:19:11.210851: step 11700, loss = 0.90, accuracy = 0.80 (327.9 examples/sec; 0.390 sec/batch)
2017-05-11 08:19:47.540643: step 11800, loss = 0.87, accuracy = 0.77 (330.3 examples/sec; 0.388 sec/batch)
2017-05-11 08:20:24.533535: step 11900, loss = 1.02, accuracy = 0.71 (292.8 examples/sec; 0.437 sec/batch)
2017-05-11 08:21:01.388750: step 12000, loss = 0.81, accuracy = 0.78 (360.1 examples/sec; 0.355 sec/batch)
2017-05-11 08:21:39.416383: step 12100, loss = 0.78, accuracy = 0.82 (331.2 examples/sec; 0.386 sec/batch)
2017-05-11 08:22:17.716578: step 12200, loss = 0.88, accuracy = 0.78 (313.4 examples/sec; 0.408 sec/batch)
2017-05-11 08:22:55.438619: step 12300, loss = 0.73, accuracy = 0.82 (603.7 examples/sec; 0.212 sec/batch)
2017-05-11 08:23:32.866684: step 12400, loss = 0.94, accuracy = 0.76 (418.0 examples/sec; 0.306 sec/batch)
2017-05-11 08:24:10.494510: step 12500, loss = 0.84, accuracy = 0.80 (399.3 examples/sec; 0.321 sec/batch)
2017-05-11 08:24:48.384464: step 12600, loss = 0.84, accuracy = 0.82 (338.7 examples/sec; 0.378 sec/batch)
2017-05-11 08:25:24.227464: step 12700, loss = 0.82, accuracy = 0.73 (379.9 examples/sec; 0.337 sec/batch)
2017-05-11 08:26:00.877945: step 12800, loss = 0.76, accuracy = 0.81 (323.8 examples/sec; 0.395 sec/batch)
2017-05-11 08:26:39.804594: step 12900, loss = 0.89, accuracy = 0.73 (321.8 examples/sec; 0.398 sec/batch)
2017-05-11 08:27:17.489148: step 13000, loss = 1.05, accuracy = 0.72 (406.4 examples/sec; 0.315 sec/batch)
2017-05-11 08:27:54.801472: step 13100, loss = 0.74, accuracy = 0.81 (469.2 examples/sec; 0.273 sec/batch)
2017-05-11 08:28:32.165341: step 13200, loss = 0.98, accuracy = 0.70 (349.8 examples/sec; 0.366 sec/batch)
2017-05-11 08:29:09.188206: step 13300, loss = 0.83, accuracy = 0.77 (505.3 examples/sec; 0.253 sec/batch)
2017-05-11 08:29:46.047753: step 13400, loss = 0.83, accuracy = 0.74 (377.9 examples/sec; 0.339 sec/batch)
2017-05-11 08:30:23.343844: step 13500, loss = 0.95, accuracy = 0.75 (590.1 examples/sec; 0.217 sec/batch)
2017-05-11 08:30:59.242275: step 13600, loss = 0.92, accuracy = 0.77 (316.4 examples/sec; 0.405 sec/batch)
2017-05-11 08:31:36.345971: step 13700, loss = 0.93, accuracy = 0.78 (492.2 examples/sec; 0.260 sec/batch)
2017-05-11 08:32:12.778461: step 13800, loss = 0.74, accuracy = 0.83 (332.7 examples/sec; 0.385 sec/batch)
2017-05-11 08:32:49.024407: step 13900, loss = 0.77, accuracy = 0.80 (327.4 examples/sec; 0.391 sec/batch)
2017-05-11 08:33:25.768954: step 14000, loss = 0.80, accuracy = 0.78 (331.4 examples/sec; 0.386 sec/batch)
2017-05-11 08:34:02.678795: step 14100, loss = 0.84, accuracy = 0.80 (370.9 examples/sec; 0.345 sec/batch)
2017-05-11 08:34:39.867683: step 14200, loss = 0.83, accuracy = 0.80 (303.5 examples/sec; 0.422 sec/batch)
2017-05-11 08:35:17.561796: step 14300, loss = 0.92, accuracy = 0.75 (403.5 examples/sec; 0.317 sec/batch)
2017-05-11 08:35:54.456833: step 14400, loss = 0.76, accuracy = 0.78 (455.8 examples/sec; 0.281 sec/batch)
2017-05-11 08:36:30.061562: step 14500, loss = 0.79, accuracy = 0.78 (405.0 examples/sec; 0.316 sec/batch)
2017-05-11 08:37:06.394582: step 14600, loss = 1.06, accuracy = 0.75 (401.2 examples/sec; 0.319 sec/batch)
2017-05-11 08:37:42.609790: step 14700, loss = 0.94, accuracy = 0.72 (325.5 examples/sec; 0.393 sec/batch)
2017-05-11 08:38:18.997443: step 14800, loss = 0.84, accuracy = 0.79 (350.4 examples/sec; 0.365 sec/batch)
2017-05-11 08:38:56.407005: step 14900, loss = 0.75, accuracy = 0.82 (270.8 examples/sec; 0.473 sec/batch)
2017-05-11 08:39:31.674793: step 15000, loss = 0.88, accuracy = 0.75 (394.3 examples/sec; 0.325 sec/batch)
2017-05-11 08:40:07.923934: step 15100, loss = 0.89, accuracy = 0.76 (292.2 examples/sec; 0.438 sec/batch)
2017-05-11 08:40:45.800583: step 15200, loss = 1.00, accuracy = 0.70 (337.0 examples/sec; 0.380 sec/batch)
2017-05-11 08:41:22.900341: step 15300, loss = 0.69, accuracy = 0.83 (292.5 examples/sec; 0.438 sec/batch)
2017-05-11 08:42:01.318567: step 15400, loss = 0.71, accuracy = 0.81 (325.3 examples/sec; 0.394 sec/batch)
2017-05-11 08:42:39.444664: step 15500, loss = 0.64, accuracy = 0.87 (364.2 examples/sec; 0.351 sec/batch)
2017-05-11 08:43:17.472872: step 15600, loss = 0.96, accuracy = 0.78 (303.2 examples/sec; 0.422 sec/batch)
2017-05-11 08:43:53.659370: step 15700, loss = 0.83, accuracy = 0.78 (335.9 examples/sec; 0.381 sec/batch)
2017-05-11 08:44:31.964920: step 15800, loss = 0.93, accuracy = 0.77 (369.3 examples/sec; 0.347 sec/batch)
2017-05-11 08:45:07.771669: step 15900, loss = 0.87, accuracy = 0.77 (473.1 examples/sec; 0.271 sec/batch)
2017-05-11 08:45:45.271555: step 16000, loss = 0.78, accuracy = 0.80 (347.7 examples/sec; 0.368 sec/batch)
2017-05-11 08:46:23.337718: step 16100, loss = 0.85, accuracy = 0.79 (319.7 examples/sec; 0.400 sec/batch)
2017-05-11 08:46:59.954591: step 16200, loss = 0.89, accuracy = 0.78 (306.2 examples/sec; 0.418 sec/batch)
2017-05-11 08:47:37.807240: step 16300, loss = 0.87, accuracy = 0.74 (329.7 examples/sec; 0.388 sec/batch)
2017-05-11 08:48:14.125606: step 16400, loss = 0.94, accuracy = 0.77 (322.9 examples/sec; 0.396 sec/batch)
2017-05-11 08:48:49.763219: step 16500, loss = 0.74, accuracy = 0.83 (385.5 examples/sec; 0.332 sec/batch)
2017-05-11 08:49:27.032217: step 16600, loss = 0.89, accuracy = 0.72 (334.5 examples/sec; 0.383 sec/batch)
2017-05-11 08:50:04.970206: step 16700, loss = 0.72, accuracy = 0.86 (429.2 examples/sec; 0.298 sec/batch)
2017-05-11 08:50:42.202122: step 16800, loss = 0.83, accuracy = 0.80 (327.6 examples/sec; 0.391 sec/batch)
2017-05-11 08:51:19.331184: step 16900, loss = 0.89, accuracy = 0.76 (325.7 examples/sec; 0.393 sec/batch)
2017-05-11 08:51:55.084797: step 17000, loss = 0.86, accuracy = 0.79 (507.8 examples/sec; 0.252 sec/batch)
2017-05-11 08:52:32.864396: step 17100, loss = 1.12, accuracy = 0.66 (305.4 examples/sec; 0.419 sec/batch)
2017-05-11 08:53:09.342127: step 17200, loss = 0.77, accuracy = 0.80 (299.4 examples/sec; 0.427 sec/batch)
2017-05-11 08:53:47.049128: step 17300, loss = 0.85, accuracy = 0.80 (325.0 examples/sec; 0.394 sec/batch)
2017-05-11 08:54:24.368063: step 17400, loss = 0.80, accuracy = 0.80 (406.6 examples/sec; 0.315 sec/batch)
2017-05-11 08:55:02.249337: step 17500, loss = 0.91, accuracy = 0.79 (406.0 examples/sec; 0.315 sec/batch)
2017-05-11 08:55:39.694280: step 17600, loss = 0.94, accuracy = 0.74 (486.9 examples/sec; 0.263 sec/batch)
2017-05-11 08:56:17.369612: step 17700, loss = 0.80, accuracy = 0.80 (336.8 examples/sec; 0.380 sec/batch)
2017-05-11 08:56:53.443923: step 17800, loss = 0.79, accuracy = 0.80 (431.8 examples/sec; 0.296 sec/batch)
2017-05-11 08:57:30.187358: step 17900, loss = 0.78, accuracy = 0.78 (338.4 examples/sec; 0.378 sec/batch)
2017-05-11 08:58:06.969526: step 18000, loss = 0.98, accuracy = 0.78 (315.0 examples/sec; 0.406 sec/batch)
2017-05-11 08:58:44.301162: step 18100, loss = 0.80, accuracy = 0.80 (390.1 examples/sec; 0.328 sec/batch)
2017-05-11 08:59:20.810037: step 18200, loss = 0.86, accuracy = 0.74 (378.6 examples/sec; 0.338 sec/batch)
2017-05-11 08:59:58.745242: step 18300, loss = 0.82, accuracy = 0.76 (417.3 examples/sec; 0.307 sec/batch)
2017-05-11 09:00:36.560215: step 18400, loss = 0.66, accuracy = 0.80 (385.8 examples/sec; 0.332 sec/batch)
2017-05-11 09:01:13.055264: step 18500, loss = 0.77, accuracy = 0.81 (330.2 examples/sec; 0.388 sec/batch)
2017-05-11 09:01:50.171157: step 18600, loss = 0.88, accuracy = 0.77 (343.0 examples/sec; 0.373 sec/batch)
2017-05-11 09:02:26.990217: step 18700, loss = 0.89, accuracy = 0.77 (412.5 examples/sec; 0.310 sec/batch)
2017-05-11 09:03:04.478244: step 18800, loss = 0.92, accuracy = 0.74 (307.6 examples/sec; 0.416 sec/batch)
2017-05-11 09:03:42.632423: step 18900, loss = 1.03, accuracy = 0.73 (284.4 examples/sec; 0.450 sec/batch)
2017-05-11 09:04:21.739931: step 19000, loss = 0.71, accuracy = 0.84 (363.9 examples/sec; 0.352 sec/batch)
2017-05-11 09:04:59.096870: step 19100, loss = 0.91, accuracy = 0.77 (294.2 examples/sec; 0.435 sec/batch)
2017-05-11 09:05:36.530977: step 19200, loss = 0.83, accuracy = 0.78 (541.6 examples/sec; 0.236 sec/batch)
2017-05-11 09:06:13.575606: step 19300, loss = 0.88, accuracy = 0.76 (368.4 examples/sec; 0.347 sec/batch)
2017-05-11 09:06:51.069290: step 19400, loss = 0.81, accuracy = 0.83 (561.8 examples/sec; 0.228 sec/batch)
2017-05-11 09:07:27.307549: step 19500, loss = 0.92, accuracy = 0.76 (351.8 examples/sec; 0.364 sec/batch)
2017-05-11 09:08:03.870321: step 19600, loss = 0.82, accuracy = 0.80 (490.9 examples/sec; 0.261 sec/batch)
2017-05-11 09:08:41.760134: step 19700, loss = 0.84, accuracy = 0.79 (302.6 examples/sec; 0.423 sec/batch)
2017-05-11 09:09:18.505966: step 19800, loss = 0.79, accuracy = 0.80 (473.0 examples/sec; 0.271 sec/batch)
2017-05-11 09:09:55.147296: step 19900, loss = 0.87, accuracy = 0.80 (332.2 examples/sec; 0.385 sec/batch)
2017-05-11 09:10:33.158625: step 20000, loss = 0.93, accuracy = 0.77 (319.2 examples/sec; 0.401 sec/batch)
2017-05-11 09:11:08.539123: step 20100, loss = 0.88, accuracy = 0.77 (370.4 examples/sec; 0.346 sec/batch)
2017-05-11 09:11:46.234458: step 20200, loss = 1.02, accuracy = 0.73 (404.8 examples/sec; 0.316 sec/batch)
2017-05-11 09:12:23.180340: step 20300, loss = 0.76, accuracy = 0.80 (408.7 examples/sec; 0.313 sec/batch)
2017-05-11 09:12:59.076733: step 20400, loss = 0.91, accuracy = 0.75 (416.7 examples/sec; 0.307 sec/batch)
2017-05-11 09:13:36.275644: step 20500, loss = 0.83, accuracy = 0.76 (416.6 examples/sec; 0.307 sec/batch)
2017-05-11 09:14:12.570974: step 20600, loss = 0.84, accuracy = 0.75 (349.1 examples/sec; 0.367 sec/batch)
2017-05-11 09:14:48.558168: step 20700, loss = 0.85, accuracy = 0.85 (465.6 examples/sec; 0.275 sec/batch)
2017-05-11 09:15:25.862566: step 20800, loss = 1.05, accuracy = 0.71 (343.3 examples/sec; 0.373 sec/batch)
2017-05-11 09:16:02.656453: step 20900, loss = 0.83, accuracy = 0.82 (327.0 examples/sec; 0.391 sec/batch)
2017-05-11 09:16:40.011350: step 21000, loss = 0.72, accuracy = 0.84 (298.0 examples/sec; 0.430 sec/batch)
2017-05-11 09:17:17.189372: step 21100, loss = 0.77, accuracy = 0.78 (474.3 examples/sec; 0.270 sec/batch)
2017-05-11 09:17:54.211651: step 21200, loss = 0.80, accuracy = 0.76 (350.0 examples/sec; 0.366 sec/batch)
2017-05-11 09:18:32.559769: step 21300, loss = 0.83, accuracy = 0.78 (323.7 examples/sec; 0.395 sec/batch)
2017-05-11 09:19:11.253983: step 21400, loss = 0.91, accuracy = 0.75 (370.1 examples/sec; 0.346 sec/batch)
2017-05-11 09:19:47.036816: step 21500, loss = 0.78, accuracy = 0.80 (406.3 examples/sec; 0.315 sec/batch)
2017-05-11 09:20:24.337612: step 21600, loss = 0.70, accuracy = 0.84 (331.6 examples/sec; 0.386 sec/batch)
2017-05-11 09:21:01.098646: step 21700, loss = 0.77, accuracy = 0.78 (332.7 examples/sec; 0.385 sec/batch)
2017-05-11 09:21:38.845046: step 21800, loss = 0.85, accuracy = 0.75 (446.1 examples/sec; 0.287 sec/batch)
2017-05-11 09:22:16.285244: step 21900, loss = 0.87, accuracy = 0.77 (316.0 examples/sec; 0.405 sec/batch)
2017-05-11 09:22:53.481723: step 22000, loss = 0.92, accuracy = 0.73 (311.5 examples/sec; 0.411 sec/batch)
2017-05-11 09:23:31.221522: step 22100, loss = 0.80, accuracy = 0.80 (407.4 examples/sec; 0.314 sec/batch)
2017-05-11 09:24:06.600662: step 22200, loss = 0.80, accuracy = 0.80 (463.0 examples/sec; 0.276 sec/batch)
2017-05-11 09:24:44.537575: step 22300, loss = 0.82, accuracy = 0.77 (256.7 examples/sec; 0.499 sec/batch)
2017-05-11 09:25:21.568753: step 22400, loss = 0.71, accuracy = 0.84 (293.9 examples/sec; 0.436 sec/batch)
2017-05-11 09:26:00.069295: step 22500, loss = 0.90, accuracy = 0.78 (275.7 examples/sec; 0.464 sec/batch)
2017-05-11 09:26:37.094213: step 22600, loss = 0.82, accuracy = 0.79 (259.1 examples/sec; 0.494 sec/batch)
2017-05-11 09:27:14.404169: step 22700, loss = 0.93, accuracy = 0.71 (269.1 examples/sec; 0.476 sec/batch)
2017-05-11 09:27:52.657539: step 22800, loss = 0.84, accuracy = 0.78 (279.7 examples/sec; 0.458 sec/batch)
2017-05-11 09:28:29.995432: step 22900, loss = 0.80, accuracy = 0.78 (375.6 examples/sec; 0.341 sec/batch)
2017-05-11 09:29:07.728052: step 23000, loss = 0.84, accuracy = 0.76 (275.9 examples/sec; 0.464 sec/batch)
2017-05-11 09:29:42.970896: step 23100, loss = 0.65, accuracy = 0.86 (367.9 examples/sec; 0.348 sec/batch)
2017-05-11 09:30:19.607329: step 23200, loss = 0.82, accuracy = 0.80 (357.2 examples/sec; 0.358 sec/batch)
2017-05-11 09:30:55.470709: step 23300, loss = 0.72, accuracy = 0.79 (419.7 examples/sec; 0.305 sec/batch)
2017-05-11 09:31:32.191075: step 23400, loss = 0.71, accuracy = 0.85 (589.6 examples/sec; 0.217 sec/batch)
2017-05-11 09:32:09.730804: step 23500, loss = 0.76, accuracy = 0.85 (329.6 examples/sec; 0.388 sec/batch)
2017-05-11 09:32:46.896634: step 23600, loss = 0.87, accuracy = 0.77 (567.0 examples/sec; 0.226 sec/batch)
2017-05-11 09:33:25.209566: step 23700, loss = 0.79, accuracy = 0.82 (283.9 examples/sec; 0.451 sec/batch)
2017-05-11 09:34:02.245089: step 23800, loss = 0.85, accuracy = 0.80 (277.3 examples/sec; 0.462 sec/batch)
2017-05-11 09:34:39.143542: step 23900, loss = 1.04, accuracy = 0.76 (295.7 examples/sec; 0.433 sec/batch)
2017-05-11 09:35:16.585556: step 24000, loss = 0.75, accuracy = 0.82 (363.6 examples/sec; 0.352 sec/batch)
2017-05-11 09:35:53.071628: step 24100, loss = 0.81, accuracy = 0.77 (423.5 examples/sec; 0.302 sec/batch)
2017-05-11 09:36:29.187662: step 24200, loss = 0.80, accuracy = 0.80 (473.0 examples/sec; 0.271 sec/batch)
2017-05-11 09:37:06.966726: step 24300, loss = 0.85, accuracy = 0.76 (309.2 examples/sec; 0.414 sec/batch)
2017-05-11 09:37:44.982699: step 24400, loss = 0.91, accuracy = 0.74 (297.8 examples/sec; 0.430 sec/batch)
2017-05-11 09:38:20.052546: step 24500, loss = 0.91, accuracy = 0.77 (326.0 examples/sec; 0.393 sec/batch)
2017-05-11 09:38:57.495724: step 24600, loss = 0.84, accuracy = 0.77 (294.8 examples/sec; 0.434 sec/batch)
2017-05-11 09:39:34.824328: step 24700, loss = 0.78, accuracy = 0.85 (397.3 examples/sec; 0.322 sec/batch)
2017-05-11 09:40:11.825393: step 24800, loss = 0.85, accuracy = 0.79 (405.5 examples/sec; 0.316 sec/batch)
2017-05-11 09:40:49.983192: step 24900, loss = 0.65, accuracy = 0.88 (322.6 examples/sec; 0.397 sec/batch)
2017-05-11 09:41:26.307214: step 25000, loss = 0.85, accuracy = 0.78 (338.6 examples/sec; 0.378 sec/batch)
2017-05-11 09:42:05.894283: step 25100, loss = 0.98, accuracy = 0.80 (312.6 examples/sec; 0.410 sec/batch)
2017-05-11 09:42:43.312059: step 25200, loss = 0.79, accuracy = 0.79 (900.7 examples/sec; 0.142 sec/batch)
2017-05-11 09:43:21.755400: step 25300, loss = 0.78, accuracy = 0.84 (447.5 examples/sec; 0.286 sec/batch)
2017-05-11 09:43:57.792333: step 25400, loss = 0.84, accuracy = 0.75 (362.5 examples/sec; 0.353 sec/batch)
2017-05-11 09:44:35.539680: step 25500, loss = 0.88, accuracy = 0.77 (313.7 examples/sec; 0.408 sec/batch)
2017-05-11 09:45:11.678360: step 25600, loss = 0.88, accuracy = 0.77 (312.5 examples/sec; 0.410 sec/batch)
2017-05-11 09:45:51.258993: step 25700, loss = 0.78, accuracy = 0.80 (265.1 examples/sec; 0.483 sec/batch)
2017-05-11 09:46:26.777875: step 25800, loss = 0.72, accuracy = 0.82 (348.0 examples/sec; 0.368 sec/batch)
2017-05-11 09:47:05.819545: step 25900, loss = 0.77, accuracy = 0.80 (311.9 examples/sec; 0.410 sec/batch)
2017-05-11 09:47:43.632095: step 26000, loss = 0.73, accuracy = 0.84 (294.9 examples/sec; 0.434 sec/batch)
2017-05-11 09:48:20.363814: step 26100, loss = 0.82, accuracy = 0.82 (370.7 examples/sec; 0.345 sec/batch)
2017-05-11 09:48:56.855284: step 26200, loss = 0.67, accuracy = 0.85 (283.0 examples/sec; 0.452 sec/batch)
2017-05-11 09:49:34.625408: step 26300, loss = 0.87, accuracy = 0.78 (335.5 examples/sec; 0.382 sec/batch)
2017-05-11 09:50:12.711812: step 26400, loss = 0.77, accuracy = 0.81 (278.0 examples/sec; 0.461 sec/batch)
2017-05-11 09:50:50.119179: step 26500, loss = 0.85, accuracy = 0.74 (324.2 examples/sec; 0.395 sec/batch)
2017-05-11 09:51:27.195317: step 26600, loss = 0.77, accuracy = 0.77 (410.9 examples/sec; 0.312 sec/batch)
2017-05-11 09:52:02.644081: step 26700, loss = 0.91, accuracy = 0.80 (381.3 examples/sec; 0.336 sec/batch)
2017-05-11 09:52:40.489004: step 26800, loss = 0.63, accuracy = 0.84 (285.3 examples/sec; 0.449 sec/batch)
2017-05-11 09:53:18.820973: step 26900, loss = 0.75, accuracy = 0.80 (303.4 examples/sec; 0.422 sec/batch)
2017-05-11 09:53:55.737661: step 27000, loss = 0.82, accuracy = 0.80 (386.8 examples/sec; 0.331 sec/batch)
2017-05-11 09:54:31.198989: step 27100, loss = 0.83, accuracy = 0.81 (304.4 examples/sec; 0.421 sec/batch)
2017-05-11 09:55:10.227644: step 27200, loss = 0.68, accuracy = 0.84 (313.8 examples/sec; 0.408 sec/batch)
2017-05-11 09:55:48.373831: step 27300, loss = 0.73, accuracy = 0.80 (425.7 examples/sec; 0.301 sec/batch)
2017-05-11 09:56:25.582891: step 27400, loss = 0.65, accuracy = 0.84 (396.9 examples/sec; 0.323 sec/batch)
2017-05-11 09:57:01.515624: step 27500, loss = 0.93, accuracy = 0.74 (379.9 examples/sec; 0.337 sec/batch)
2017-05-11 09:57:39.989699: step 27600, loss = 0.84, accuracy = 0.80 (391.4 examples/sec; 0.327 sec/batch)
2017-05-11 09:58:18.503502: step 27700, loss = 0.79, accuracy = 0.81 (361.7 examples/sec; 0.354 sec/batch)
2017-05-11 09:58:57.091372: step 27800, loss = 0.96, accuracy = 0.77 (434.6 examples/sec; 0.295 sec/batch)
2017-05-11 09:59:33.810793: step 27900, loss = 0.85, accuracy = 0.78 (896.9 examples/sec; 0.143 sec/batch)
2017-05-11 10:00:09.738936: step 28000, loss = 0.77, accuracy = 0.80 (394.9 examples/sec; 0.324 sec/batch)
2017-05-11 10:00:47.794144: step 28100, loss = 0.92, accuracy = 0.77 (370.7 examples/sec; 0.345 sec/batch)
2017-05-11 10:01:25.526555: step 28200, loss = 0.79, accuracy = 0.80 (282.4 examples/sec; 0.453 sec/batch)
2017-05-11 10:02:02.436521: step 28300, loss = 0.90, accuracy = 0.74 (350.1 examples/sec; 0.366 sec/batch)
2017-05-11 10:02:39.380704: step 28400, loss = 0.94, accuracy = 0.79 (338.6 examples/sec; 0.378 sec/batch)
2017-05-11 10:03:16.349145: step 28500, loss = 0.71, accuracy = 0.84 (545.2 examples/sec; 0.235 sec/batch)
2017-05-11 10:03:52.447532: step 28600, loss = 0.84, accuracy = 0.77 (406.4 examples/sec; 0.315 sec/batch)
2017-05-11 10:04:28.633248: step 28700, loss = 0.67, accuracy = 0.82 (360.4 examples/sec; 0.355 sec/batch)
2017-05-11 10:05:06.550853: step 28800, loss = 0.87, accuracy = 0.79 (333.6 examples/sec; 0.384 sec/batch)
2017-05-11 10:05:43.134504: step 28900, loss = 0.73, accuracy = 0.84 (294.8 examples/sec; 0.434 sec/batch)
2017-05-11 10:06:21.100080: step 29000, loss = 0.78, accuracy = 0.79 (262.1 examples/sec; 0.488 sec/batch)
2017-05-11 10:06:57.789152: step 29100, loss = 0.69, accuracy = 0.81 (356.0 examples/sec; 0.360 sec/batch)
2017-05-11 10:07:35.447352: step 29200, loss = 0.77, accuracy = 0.80 (365.1 examples/sec; 0.351 sec/batch)
2017-05-11 10:08:10.931557: step 29300, loss = 0.84, accuracy = 0.81 (468.0 examples/sec; 0.273 sec/batch)
2017-05-11 10:08:47.869356: step 29400, loss = 0.67, accuracy = 0.84 (379.1 examples/sec; 0.338 sec/batch)
2017-05-11 10:09:25.780776: step 29500, loss = 1.02, accuracy = 0.75 (658.6 examples/sec; 0.194 sec/batch)
2017-05-11 10:10:01.840018: step 29600, loss = 0.81, accuracy = 0.76 (347.8 examples/sec; 0.368 sec/batch)
2017-05-11 10:10:36.770056: step 29700, loss = 0.74, accuracy = 0.85 (315.6 examples/sec; 0.406 sec/batch)
2017-05-11 10:11:13.520483: step 29800, loss = 0.90, accuracy = 0.73 (254.5 examples/sec; 0.503 sec/batch)
2017-05-11 10:11:50.372590: step 29900, loss = 0.88, accuracy = 0.82 (385.8 examples/sec; 0.332 sec/batch)
2017-05-11 10:12:27.841743: step 30000, loss = 0.74, accuracy = 0.84 (284.3 examples/sec; 0.450 sec/batch)

 Average train accuracy: 0.7466
 Max train accuracy: 0.875
