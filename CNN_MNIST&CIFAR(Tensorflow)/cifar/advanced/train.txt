2017-05-11 10:44:45.231860: step 0, loss = 5.54, accuracy = 0.14 (2.3 examples/sec; 55.141 sec/batch)
2017-05-11 10:45:23.832945: step 100, loss = 4.82, accuracy = 0.34 (437.1 examples/sec; 0.293 sec/batch)
2017-05-11 10:46:04.448431: step 200, loss = 4.54, accuracy = 0.34 (300.0 examples/sec; 0.427 sec/batch)
2017-05-11 10:46:44.754192: step 300, loss = 4.13, accuracy = 0.41 (371.1 examples/sec; 0.345 sec/batch)
2017-05-11 10:47:24.220833: step 400, loss = 3.95, accuracy = 0.39 (448.1 examples/sec; 0.286 sec/batch)
2017-05-11 10:48:02.572341: step 500, loss = 3.91, accuracy = 0.36 (426.4 examples/sec; 0.300 sec/batch)
2017-05-11 10:48:42.775536: step 600, loss = 3.22, accuracy = 0.57 (291.2 examples/sec; 0.440 sec/batch)
2017-05-11 10:49:22.553928: step 700, loss = 3.14, accuracy = 0.60 (296.9 examples/sec; 0.431 sec/batch)
2017-05-11 10:50:02.769741: step 800, loss = 3.03, accuracy = 0.55 (358.2 examples/sec; 0.357 sec/batch)
2017-05-11 10:50:40.842909: step 900, loss = 2.94, accuracy = 0.51 (308.8 examples/sec; 0.414 sec/batch)
2017-05-11 10:51:19.564774: step 1000, loss = 2.77, accuracy = 0.56 (340.3 examples/sec; 0.376 sec/batch)
2017-05-11 10:51:57.898074: step 1100, loss = 2.59, accuracy = 0.55 (334.4 examples/sec; 0.383 sec/batch)
2017-05-11 10:52:39.660122: step 1200, loss = 2.42, accuracy = 0.67 (269.8 examples/sec; 0.474 sec/batch)
2017-05-11 10:53:17.721941: step 1300, loss = 2.35, accuracy = 0.55 (281.5 examples/sec; 0.455 sec/batch)
2017-05-11 10:53:57.871514: step 1400, loss = 2.29, accuracy = 0.59 (464.9 examples/sec; 0.275 sec/batch)
2017-05-11 10:54:37.536960: step 1500, loss = 1.81, accuracy = 0.75 (317.9 examples/sec; 0.403 sec/batch)
2017-05-11 10:55:17.807911: step 1600, loss = 2.04, accuracy = 0.64 (415.7 examples/sec; 0.308 sec/batch)
2017-05-11 10:55:56.775698: step 1700, loss = 1.93, accuracy = 0.60 (307.9 examples/sec; 0.416 sec/batch)
2017-05-11 10:56:35.632356: step 1800, loss = 1.90, accuracy = 0.62 (328.1 examples/sec; 0.390 sec/batch)
2017-05-11 10:57:14.789960: step 1900, loss = 1.80, accuracy = 0.62 (445.4 examples/sec; 0.287 sec/batch)
2017-05-11 10:57:53.266697: step 2000, loss = 1.79, accuracy = 0.66 (1067.4 examples/sec; 0.120 sec/batch)
2017-05-11 10:58:33.353087: step 2100, loss = 1.79, accuracy = 0.59 (438.3 examples/sec; 0.292 sec/batch)
2017-05-11 10:59:12.024794: step 2200, loss = 1.62, accuracy = 0.69 (335.2 examples/sec; 0.382 sec/batch)
2017-05-11 10:59:50.919879: step 2300, loss = 1.48, accuracy = 0.73 (287.9 examples/sec; 0.445 sec/batch)
2017-05-11 11:00:31.547381: step 2400, loss = 1.54, accuracy = 0.68 (291.0 examples/sec; 0.440 sec/batch)
2017-05-11 11:01:10.234621: step 2500, loss = 1.47, accuracy = 0.72 (377.7 examples/sec; 0.339 sec/batch)
2017-05-11 11:01:49.312774: step 2600, loss = 1.69, accuracy = 0.61 (382.6 examples/sec; 0.335 sec/batch)
2017-05-11 11:02:30.038930: step 2700, loss = 1.58, accuracy = 0.62 (341.3 examples/sec; 0.375 sec/batch)
2017-05-11 11:03:10.256780: step 2800, loss = 1.35, accuracy = 0.70 (283.6 examples/sec; 0.451 sec/batch)
2017-05-11 11:03:48.420750: step 2900, loss = 1.45, accuracy = 0.66 (296.1 examples/sec; 0.432 sec/batch)
2017-05-11 11:04:26.891744: step 3000, loss = 1.23, accuracy = 0.71 (345.8 examples/sec; 0.370 sec/batch)
2017-05-11 11:05:04.699243: step 3100, loss = 1.23, accuracy = 0.72 (380.6 examples/sec; 0.336 sec/batch)
2017-05-11 11:05:44.491559: step 3200, loss = 1.33, accuracy = 0.66 (322.1 examples/sec; 0.397 sec/batch)
2017-05-11 11:06:25.687998: step 3300, loss = 1.37, accuracy = 0.64 (496.6 examples/sec; 0.258 sec/batch)
2017-05-11 11:07:04.485100: step 3400, loss = 1.26, accuracy = 0.66 (320.9 examples/sec; 0.399 sec/batch)
2017-05-11 11:07:43.316530: step 3500, loss = 1.29, accuracy = 0.70 (265.9 examples/sec; 0.481 sec/batch)
2017-05-11 11:08:23.381427: step 3600, loss = 1.22, accuracy = 0.66 (304.4 examples/sec; 0.420 sec/batch)
2017-05-11 11:09:02.200924: step 3700, loss = 1.23, accuracy = 0.69 (557.1 examples/sec; 0.230 sec/batch)
2017-05-11 11:09:42.141038: step 3800, loss = 1.24, accuracy = 0.58 (302.3 examples/sec; 0.423 sec/batch)
2017-05-11 11:10:23.121940: step 3900, loss = 1.04, accuracy = 0.77 (348.9 examples/sec; 0.367 sec/batch)
2017-05-11 11:11:01.323678: step 4000, loss = 1.28, accuracy = 0.62 (295.9 examples/sec; 0.433 sec/batch)
2017-05-11 11:11:38.681282: step 4100, loss = 1.31, accuracy = 0.62 (854.9 examples/sec; 0.150 sec/batch)
2017-05-11 11:12:18.255725: step 4200, loss = 1.08, accuracy = 0.74 (503.5 examples/sec; 0.254 sec/batch)
2017-05-11 11:12:58.002729: step 4300, loss = 1.11, accuracy = 0.66 (313.8 examples/sec; 0.408 sec/batch)
2017-05-11 11:13:38.344092: step 4400, loss = 1.19, accuracy = 0.62 (301.8 examples/sec; 0.424 sec/batch)
2017-05-11 11:14:19.165784: step 4500, loss = 1.13, accuracy = 0.65 (315.1 examples/sec; 0.406 sec/batch)
2017-05-11 11:14:57.956409: step 4600, loss = 1.08, accuracy = 0.68 (440.3 examples/sec; 0.291 sec/batch)
2017-05-11 11:15:37.935950: step 4700, loss = 0.95, accuracy = 0.74 (293.0 examples/sec; 0.437 sec/batch)
2017-05-11 11:16:17.404053: step 4800, loss = 1.01, accuracy = 0.69 (400.0 examples/sec; 0.320 sec/batch)
2017-05-11 11:16:56.815758: step 4900, loss = 1.12, accuracy = 0.66 (487.8 examples/sec; 0.262 sec/batch)
2017-05-11 11:17:36.364788: step 5000, loss = 1.00, accuracy = 0.75 (263.5 examples/sec; 0.486 sec/batch)
2017-05-11 11:18:14.637285: step 5100, loss = 1.07, accuracy = 0.73 (353.4 examples/sec; 0.362 sec/batch)
2017-05-11 11:18:53.548176: step 5200, loss = 0.88, accuracy = 0.77 (403.4 examples/sec; 0.317 sec/batch)
2017-05-11 11:19:33.606743: step 5300, loss = 1.13, accuracy = 0.70 (288.4 examples/sec; 0.444 sec/batch)
2017-05-11 11:20:13.496850: step 5400, loss = 1.10, accuracy = 0.67 (375.3 examples/sec; 0.341 sec/batch)
2017-05-11 11:20:53.518641: step 5500, loss = 1.01, accuracy = 0.64 (344.8 examples/sec; 0.371 sec/batch)
2017-05-11 11:21:33.746384: step 5600, loss = 0.80, accuracy = 0.78 (279.0 examples/sec; 0.459 sec/batch)
2017-05-11 11:22:12.790202: step 5700, loss = 0.98, accuracy = 0.70 (310.6 examples/sec; 0.412 sec/batch)
2017-05-11 11:22:51.562696: step 5800, loss = 0.92, accuracy = 0.73 (524.9 examples/sec; 0.244 sec/batch)
2017-05-11 11:23:31.735185: step 5900, loss = 0.77, accuracy = 0.84 (403.8 examples/sec; 0.317 sec/batch)
2017-05-11 11:24:11.284125: step 6000, loss = 0.89, accuracy = 0.77 (328.3 examples/sec; 0.390 sec/batch)
2017-05-11 11:24:51.062247: step 6100, loss = 0.88, accuracy = 0.75 (362.5 examples/sec; 0.353 sec/batch)
2017-05-11 11:25:30.644359: step 6200, loss = 0.96, accuracy = 0.73 (299.0 examples/sec; 0.428 sec/batch)
2017-05-11 11:26:09.443436: step 6300, loss = 0.92, accuracy = 0.77 (310.1 examples/sec; 0.413 sec/batch)
2017-05-11 11:26:48.792123: step 6400, loss = 0.95, accuracy = 0.73 (361.6 examples/sec; 0.354 sec/batch)
2017-05-11 11:27:27.733111: step 6500, loss = 1.00, accuracy = 0.68 (278.6 examples/sec; 0.459 sec/batch)
2017-05-11 11:28:06.840043: step 6600, loss = 0.83, accuracy = 0.80 (344.5 examples/sec; 0.372 sec/batch)
2017-05-11 11:28:45.609763: step 6700, loss = 1.08, accuracy = 0.70 (345.9 examples/sec; 0.370 sec/batch)
2017-05-11 11:29:23.820297: step 6800, loss = 0.87, accuracy = 0.78 (322.0 examples/sec; 0.398 sec/batch)
2017-05-11 11:30:04.020024: step 6900, loss = 0.84, accuracy = 0.70 (286.5 examples/sec; 0.447 sec/batch)
2017-05-11 11:30:44.076217: step 7000, loss = 0.85, accuracy = 0.76 (452.3 examples/sec; 0.283 sec/batch)
2017-05-11 11:31:24.060125: step 7100, loss = 0.95, accuracy = 0.72 (447.3 examples/sec; 0.286 sec/batch)
2017-05-11 11:32:04.521604: step 7200, loss = 0.80, accuracy = 0.80 (303.7 examples/sec; 0.421 sec/batch)
2017-05-11 11:32:45.742376: step 7300, loss = 0.75, accuracy = 0.77 (286.3 examples/sec; 0.447 sec/batch)
2017-05-11 11:33:25.156956: step 7400, loss = 0.72, accuracy = 0.77 (332.4 examples/sec; 0.385 sec/batch)
2017-05-11 11:34:05.320041: step 7500, loss = 0.90, accuracy = 0.77 (601.8 examples/sec; 0.213 sec/batch)
2017-05-11 11:34:43.781286: step 7600, loss = 0.75, accuracy = 0.79 (321.4 examples/sec; 0.398 sec/batch)
2017-05-11 11:35:24.124364: step 7700, loss = 0.87, accuracy = 0.71 (275.7 examples/sec; 0.464 sec/batch)
2017-05-11 11:36:02.506542: step 7800, loss = 0.98, accuracy = 0.71 (269.0 examples/sec; 0.476 sec/batch)
2017-05-11 11:36:41.245065: step 7900, loss = 0.92, accuracy = 0.71 (477.4 examples/sec; 0.268 sec/batch)
2017-05-11 11:37:21.032896: step 8000, loss = 0.84, accuracy = 0.73 (332.8 examples/sec; 0.385 sec/batch)
2017-05-11 11:38:01.341792: step 8100, loss = 0.87, accuracy = 0.74 (415.8 examples/sec; 0.308 sec/batch)
2017-05-11 11:38:40.099034: step 8200, loss = 0.81, accuracy = 0.76 (299.4 examples/sec; 0.428 sec/batch)
2017-05-11 11:39:19.988088: step 8300, loss = 0.88, accuracy = 0.77 (367.7 examples/sec; 0.348 sec/batch)
2017-05-11 11:40:00.771421: step 8400, loss = 0.74, accuracy = 0.82 (261.4 examples/sec; 0.490 sec/batch)
2017-05-11 11:40:38.722239: step 8500, loss = 0.78, accuracy = 0.78 (332.9 examples/sec; 0.385 sec/batch)
2017-05-11 11:41:19.592888: step 8600, loss = 0.92, accuracy = 0.73 (323.1 examples/sec; 0.396 sec/batch)
2017-05-11 11:41:57.898250: step 8700, loss = 1.06, accuracy = 0.70 (307.8 examples/sec; 0.416 sec/batch)
2017-05-11 11:42:36.021648: step 8800, loss = 0.82, accuracy = 0.76 (368.8 examples/sec; 0.347 sec/batch)
2017-05-11 11:43:14.738216: step 8900, loss = 0.99, accuracy = 0.69 (277.8 examples/sec; 0.461 sec/batch)
2017-05-11 11:43:53.861958: step 9000, loss = 0.82, accuracy = 0.72 (236.8 examples/sec; 0.541 sec/batch)
2017-05-11 11:44:34.324645: step 9100, loss = 0.66, accuracy = 0.84 (302.7 examples/sec; 0.423 sec/batch)
2017-05-11 11:45:13.608955: step 9200, loss = 0.89, accuracy = 0.73 (365.5 examples/sec; 0.350 sec/batch)
2017-05-11 11:45:52.993267: step 9300, loss = 0.72, accuracy = 0.81 (359.4 examples/sec; 0.356 sec/batch)
2017-05-11 11:46:31.748692: step 9400, loss = 0.84, accuracy = 0.77 (290.5 examples/sec; 0.441 sec/batch)
2017-05-11 11:47:11.050095: step 9500, loss = 0.79, accuracy = 0.81 (477.4 examples/sec; 0.268 sec/batch)
2017-05-11 11:47:50.812162: step 9600, loss = 0.82, accuracy = 0.77 (401.0 examples/sec; 0.319 sec/batch)
2017-05-11 11:48:30.175283: step 9700, loss = 0.91, accuracy = 0.76 (369.4 examples/sec; 0.347 sec/batch)
2017-05-11 11:49:09.050848: step 9800, loss = 0.78, accuracy = 0.81 (722.0 examples/sec; 0.177 sec/batch)
2017-05-11 11:49:49.937193: step 9900, loss = 0.87, accuracy = 0.76 (478.7 examples/sec; 0.267 sec/batch)
2017-05-11 11:50:30.575492: step 10000, loss = 0.83, accuracy = 0.77 (315.0 examples/sec; 0.406 sec/batch)
2017-05-11 11:51:10.675404: step 10100, loss = 0.72, accuracy = 0.80 (328.2 examples/sec; 0.390 sec/batch)
2017-05-11 11:51:50.794044: step 10200, loss = 0.86, accuracy = 0.75 (291.8 examples/sec; 0.439 sec/batch)
2017-05-11 11:52:31.010913: step 10300, loss = 0.86, accuracy = 0.77 (310.7 examples/sec; 0.412 sec/batch)
2017-05-11 11:53:09.966816: step 10400, loss = 0.80, accuracy = 0.76 (357.3 examples/sec; 0.358 sec/batch)
2017-05-11 11:53:49.727125: step 10500, loss = 0.74, accuracy = 0.79 (301.2 examples/sec; 0.425 sec/batch)
2017-05-11 11:54:30.535124: step 10600, loss = 0.92, accuracy = 0.70 (466.0 examples/sec; 0.275 sec/batch)
2017-05-11 11:55:09.590865: step 10700, loss = 0.79, accuracy = 0.79 (308.9 examples/sec; 0.414 sec/batch)
2017-05-11 11:55:48.990434: step 10800, loss = 0.79, accuracy = 0.79 (285.1 examples/sec; 0.449 sec/batch)
2017-05-11 11:56:28.538460: step 10900, loss = 0.83, accuracy = 0.77 (330.6 examples/sec; 0.387 sec/batch)
2017-05-11 11:57:07.326887: step 11000, loss = 0.93, accuracy = 0.71 (307.4 examples/sec; 0.416 sec/batch)
2017-05-11 11:57:46.238649: step 11100, loss = 0.95, accuracy = 0.75 (474.0 examples/sec; 0.270 sec/batch)
2017-05-11 11:58:24.264982: step 11200, loss = 0.71, accuracy = 0.78 (353.3 examples/sec; 0.362 sec/batch)
2017-05-11 11:59:06.094922: step 11300, loss = 0.66, accuracy = 0.81 (287.3 examples/sec; 0.446 sec/batch)
2017-05-11 11:59:46.646158: step 11400, loss = 0.97, accuracy = 0.76 (271.0 examples/sec; 0.472 sec/batch)
2017-05-11 12:00:26.922510: step 11500, loss = 0.87, accuracy = 0.77 (424.8 examples/sec; 0.301 sec/batch)
2017-05-11 12:01:06.732257: step 11600, loss = 0.78, accuracy = 0.82 (315.0 examples/sec; 0.406 sec/batch)
2017-05-11 12:01:45.678289: step 11700, loss = 0.74, accuracy = 0.80 (261.8 examples/sec; 0.489 sec/batch)
2017-05-11 12:02:23.799835: step 11800, loss = 0.77, accuracy = 0.85 (337.4 examples/sec; 0.379 sec/batch)
2017-05-11 12:03:02.979159: step 11900, loss = 0.89, accuracy = 0.77 (243.4 examples/sec; 0.526 sec/batch)
2017-05-11 12:03:42.709895: step 12000, loss = 1.12, accuracy = 0.67 (317.3 examples/sec; 0.403 sec/batch)
2017-05-11 12:04:21.374148: step 12100, loss = 0.88, accuracy = 0.73 (355.6 examples/sec; 0.360 sec/batch)
2017-05-11 12:05:00.435851: step 12200, loss = 0.93, accuracy = 0.72 (381.6 examples/sec; 0.335 sec/batch)
2017-05-11 12:05:40.527680: step 12300, loss = 0.78, accuracy = 0.73 (370.5 examples/sec; 0.346 sec/batch)
2017-05-11 12:06:19.413061: step 12400, loss = 0.91, accuracy = 0.73 (403.7 examples/sec; 0.317 sec/batch)
2017-05-11 12:06:58.890466: step 12500, loss = 0.82, accuracy = 0.78 (368.4 examples/sec; 0.347 sec/batch)
2017-05-11 12:07:38.223710: step 12600, loss = 0.66, accuracy = 0.84 (390.1 examples/sec; 0.328 sec/batch)
2017-05-11 12:08:17.360649: step 12700, loss = 0.87, accuracy = 0.75 (315.5 examples/sec; 0.406 sec/batch)
2017-05-11 12:08:57.889178: step 12800, loss = 0.59, accuracy = 0.85 (571.9 examples/sec; 0.224 sec/batch)
2017-05-11 12:09:36.660983: step 12900, loss = 0.88, accuracy = 0.74 (314.3 examples/sec; 0.407 sec/batch)
2017-05-11 12:10:16.125073: step 13000, loss = 0.76, accuracy = 0.82 (418.4 examples/sec; 0.306 sec/batch)
2017-05-11 12:10:56.043813: step 13100, loss = 0.70, accuracy = 0.84 (276.3 examples/sec; 0.463 sec/batch)
2017-05-11 12:11:34.522350: step 13200, loss = 0.79, accuracy = 0.77 (370.5 examples/sec; 0.345 sec/batch)
2017-05-11 12:12:14.320421: step 13300, loss = 1.01, accuracy = 0.70 (396.9 examples/sec; 0.323 sec/batch)
2017-05-11 12:12:53.575089: step 13400, loss = 0.79, accuracy = 0.77 (293.0 examples/sec; 0.437 sec/batch)
2017-05-11 12:13:33.512805: step 13500, loss = 0.71, accuracy = 0.78 (313.3 examples/sec; 0.409 sec/batch)
2017-05-11 12:14:11.863399: step 13600, loss = 0.73, accuracy = 0.77 (433.1 examples/sec; 0.296 sec/batch)
2017-05-11 12:14:51.357459: step 13700, loss = 0.73, accuracy = 0.78 (237.4 examples/sec; 0.539 sec/batch)
2017-05-11 12:15:30.967904: step 13800, loss = 0.81, accuracy = 0.75 (302.9 examples/sec; 0.423 sec/batch)
2017-05-11 12:16:12.219986: step 13900, loss = 0.69, accuracy = 0.82 (307.5 examples/sec; 0.416 sec/batch)
2017-05-11 12:16:51.574507: step 14000, loss = 0.68, accuracy = 0.84 (306.4 examples/sec; 0.418 sec/batch)
2017-05-11 12:17:31.834038: step 14100, loss = 0.80, accuracy = 0.78 (295.0 examples/sec; 0.434 sec/batch)
2017-05-11 12:18:10.505100: step 14200, loss = 0.89, accuracy = 0.78 (391.9 examples/sec; 0.327 sec/batch)
2017-05-11 12:18:36.073225: step 14300, loss = 0.69, accuracy = 0.82 (995.1 examples/sec; 0.129 sec/batch)
2017-05-11 12:18:47.646297: step 14400, loss = 0.69, accuracy = 0.83 (1017.6 examples/sec; 0.126 sec/batch)
2017-05-11 12:18:59.028096: step 14500, loss = 0.74, accuracy = 0.80 (1194.2 examples/sec; 0.107 sec/batch)
2017-05-11 12:19:10.449426: step 14600, loss = 0.79, accuracy = 0.81 (1043.1 examples/sec; 0.123 sec/batch)
2017-05-11 12:19:22.103187: step 14700, loss = 0.72, accuracy = 0.79 (1003.2 examples/sec; 0.128 sec/batch)
2017-05-11 12:19:33.355814: step 14800, loss = 0.81, accuracy = 0.74 (937.5 examples/sec; 0.137 sec/batch)
2017-05-11 12:19:44.556624: step 14900, loss = 0.80, accuracy = 0.78 (1042.2 examples/sec; 0.123 sec/batch)
2017-05-11 12:19:56.048930: step 15000, loss = 0.76, accuracy = 0.81 (1191.0 examples/sec; 0.107 sec/batch)
2017-05-11 12:20:07.314134: step 15100, loss = 0.78, accuracy = 0.83 (956.3 examples/sec; 0.134 sec/batch)
2017-05-11 12:20:18.577387: step 15200, loss = 0.76, accuracy = 0.80 (1270.9 examples/sec; 0.101 sec/batch)
2017-05-11 12:20:29.876178: step 15300, loss = 0.77, accuracy = 0.77 (967.5 examples/sec; 0.132 sec/batch)
2017-05-11 12:20:41.174713: step 15400, loss = 0.80, accuracy = 0.77 (1224.3 examples/sec; 0.105 sec/batch)
2017-05-11 12:20:52.429000: step 15500, loss = 0.69, accuracy = 0.80 (1120.2 examples/sec; 0.114 sec/batch)
2017-05-11 12:21:03.811204: step 15600, loss = 0.80, accuracy = 0.80 (1201.7 examples/sec; 0.107 sec/batch)
2017-05-11 12:21:15.328600: step 15700, loss = 0.92, accuracy = 0.69 (1021.9 examples/sec; 0.125 sec/batch)
2017-05-11 12:21:26.411793: step 15800, loss = 0.75, accuracy = 0.81 (1216.8 examples/sec; 0.105 sec/batch)
2017-05-11 12:21:38.012285: step 15900, loss = 0.88, accuracy = 0.73 (1068.1 examples/sec; 0.120 sec/batch)
2017-05-11 12:21:49.473941: step 16000, loss = 0.82, accuracy = 0.80 (1165.1 examples/sec; 0.110 sec/batch)
2017-05-11 12:22:00.865887: step 16100, loss = 0.88, accuracy = 0.71 (1155.2 examples/sec; 0.111 sec/batch)
2017-05-11 12:22:12.167908: step 16200, loss = 0.77, accuracy = 0.80 (985.0 examples/sec; 0.130 sec/batch)
2017-05-11 12:22:23.459073: step 16300, loss = 0.82, accuracy = 0.77 (1006.6 examples/sec; 0.127 sec/batch)
2017-05-11 12:22:35.038682: step 16400, loss = 0.69, accuracy = 0.80 (1017.7 examples/sec; 0.126 sec/batch)
2017-05-11 12:22:46.297170: step 16500, loss = 0.86, accuracy = 0.80 (944.9 examples/sec; 0.135 sec/batch)
2017-05-11 12:22:57.598321: step 16600, loss = 0.76, accuracy = 0.79 (1341.3 examples/sec; 0.095 sec/batch)
2017-05-11 12:23:08.987002: step 16700, loss = 0.74, accuracy = 0.82 (1171.7 examples/sec; 0.109 sec/batch)
2017-05-11 12:23:20.227142: step 16800, loss = 0.82, accuracy = 0.78 (1208.4 examples/sec; 0.106 sec/batch)
2017-05-11 12:23:31.615647: step 16900, loss = 0.76, accuracy = 0.80 (1141.5 examples/sec; 0.112 sec/batch)
2017-05-11 12:23:42.937864: step 17000, loss = 0.65, accuracy = 0.84 (1186.4 examples/sec; 0.108 sec/batch)
2017-05-11 12:23:54.427070: step 17100, loss = 0.69, accuracy = 0.84 (1008.1 examples/sec; 0.127 sec/batch)
2017-05-11 12:24:05.884798: step 17200, loss = 0.85, accuracy = 0.80 (1029.6 examples/sec; 0.124 sec/batch)
2017-05-11 12:24:17.141086: step 17300, loss = 0.69, accuracy = 0.83 (984.8 examples/sec; 0.130 sec/batch)
2017-05-11 12:24:28.199064: step 17400, loss = 0.93, accuracy = 0.77 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-11 12:24:39.441459: step 17500, loss = 0.76, accuracy = 0.79 (1230.2 examples/sec; 0.104 sec/batch)
2017-05-11 12:24:50.801162: step 17600, loss = 0.72, accuracy = 0.79 (930.1 examples/sec; 0.138 sec/batch)
2017-05-11 12:25:02.277937: step 17700, loss = 0.91, accuracy = 0.72 (1022.9 examples/sec; 0.125 sec/batch)
2017-05-11 12:25:13.541806: step 17800, loss = 0.74, accuracy = 0.79 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-11 12:25:24.931006: step 17900, loss = 0.81, accuracy = 0.80 (1276.3 examples/sec; 0.100 sec/batch)
2017-05-11 12:25:36.271163: step 18000, loss = 0.77, accuracy = 0.77 (999.7 examples/sec; 0.128 sec/batch)
2017-05-11 12:25:47.578843: step 18100, loss = 0.69, accuracy = 0.84 (1195.7 examples/sec; 0.107 sec/batch)
2017-05-11 12:25:58.728558: step 18200, loss = 0.64, accuracy = 0.84 (1299.4 examples/sec; 0.099 sec/batch)
2017-05-11 12:26:10.132818: step 18300, loss = 0.80, accuracy = 0.80 (1340.0 examples/sec; 0.096 sec/batch)
2017-05-11 12:26:21.422972: step 18400, loss = 0.87, accuracy = 0.77 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-11 12:26:33.032731: step 18500, loss = 0.75, accuracy = 0.82 (996.5 examples/sec; 0.128 sec/batch)
2017-05-11 12:26:44.268934: step 18600, loss = 0.82, accuracy = 0.77 (1037.4 examples/sec; 0.123 sec/batch)
2017-05-11 12:26:55.637594: step 18700, loss = 0.75, accuracy = 0.84 (1142.9 examples/sec; 0.112 sec/batch)
2017-05-11 12:27:06.855131: step 18800, loss = 0.72, accuracy = 0.83 (1164.7 examples/sec; 0.110 sec/batch)
2017-05-11 12:27:18.128297: step 18900, loss = 0.77, accuracy = 0.79 (1016.0 examples/sec; 0.126 sec/batch)
2017-05-11 12:27:29.501915: step 19000, loss = 0.74, accuracy = 0.80 (1088.6 examples/sec; 0.118 sec/batch)
2017-05-11 12:27:40.773062: step 19100, loss = 0.88, accuracy = 0.73 (1054.8 examples/sec; 0.121 sec/batch)
2017-05-11 12:27:51.988991: step 19200, loss = 0.70, accuracy = 0.81 (1203.0 examples/sec; 0.106 sec/batch)
2017-05-11 12:28:03.314432: step 19300, loss = 0.84, accuracy = 0.79 (1122.7 examples/sec; 0.114 sec/batch)
2017-05-11 12:28:14.592747: step 19400, loss = 0.70, accuracy = 0.83 (1165.0 examples/sec; 0.110 sec/batch)
2017-05-11 12:28:25.990811: step 19500, loss = 0.81, accuracy = 0.78 (1138.4 examples/sec; 0.112 sec/batch)
2017-05-11 12:28:37.484249: step 19600, loss = 0.81, accuracy = 0.80 (1095.2 examples/sec; 0.117 sec/batch)
2017-05-11 12:28:48.945117: step 19700, loss = 0.69, accuracy = 0.84 (1064.3 examples/sec; 0.120 sec/batch)
2017-05-11 12:29:00.357933: step 19800, loss = 0.58, accuracy = 0.85 (1004.0 examples/sec; 0.127 sec/batch)
2017-05-11 12:29:11.571382: step 19900, loss = 0.83, accuracy = 0.82 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-11 12:29:22.881133: step 20000, loss = 0.60, accuracy = 0.86 (1086.7 examples/sec; 0.118 sec/batch)
2017-05-11 12:29:34.226937: step 20100, loss = 0.67, accuracy = 0.83 (1091.5 examples/sec; 0.117 sec/batch)
2017-05-11 12:29:45.501733: step 20200, loss = 0.72, accuracy = 0.81 (987.1 examples/sec; 0.130 sec/batch)
2017-05-11 12:29:57.040044: step 20300, loss = 0.68, accuracy = 0.86 (1084.3 examples/sec; 0.118 sec/batch)
2017-05-11 12:30:08.316223: step 20400, loss = 0.67, accuracy = 0.88 (1101.5 examples/sec; 0.116 sec/batch)
2017-05-11 12:30:19.635887: step 20500, loss = 0.72, accuracy = 0.83 (1088.1 examples/sec; 0.118 sec/batch)
2017-05-11 12:30:30.887554: step 20600, loss = 0.82, accuracy = 0.83 (1096.0 examples/sec; 0.117 sec/batch)
2017-05-11 12:30:42.358315: step 20700, loss = 0.73, accuracy = 0.84 (1273.1 examples/sec; 0.101 sec/batch)
2017-05-11 12:30:53.681954: step 20800, loss = 0.66, accuracy = 0.84 (1252.2 examples/sec; 0.102 sec/batch)
2017-05-11 12:31:05.059672: step 20900, loss = 0.78, accuracy = 0.77 (1288.6 examples/sec; 0.099 sec/batch)
2017-05-11 12:31:16.278961: step 21000, loss = 0.64, accuracy = 0.84 (1118.1 examples/sec; 0.114 sec/batch)
2017-05-11 12:31:27.797452: step 21100, loss = 0.80, accuracy = 0.79 (916.5 examples/sec; 0.140 sec/batch)
2017-05-11 12:31:39.030262: step 21200, loss = 0.78, accuracy = 0.79 (968.0 examples/sec; 0.132 sec/batch)
2017-05-11 12:31:50.334590: step 21300, loss = 0.80, accuracy = 0.80 (976.7 examples/sec; 0.131 sec/batch)
2017-05-11 12:32:01.757587: step 21400, loss = 0.70, accuracy = 0.77 (972.9 examples/sec; 0.132 sec/batch)
2017-05-11 12:32:12.992238: step 21500, loss = 0.73, accuracy = 0.80 (1234.0 examples/sec; 0.104 sec/batch)
2017-05-11 12:32:24.629097: step 21600, loss = 0.64, accuracy = 0.84 (1019.0 examples/sec; 0.126 sec/batch)
2017-05-11 12:32:35.919400: step 21700, loss = 0.77, accuracy = 0.78 (1187.9 examples/sec; 0.108 sec/batch)
2017-05-11 12:32:47.277579: step 21800, loss = 0.69, accuracy = 0.84 (1132.2 examples/sec; 0.113 sec/batch)
2017-05-11 12:32:58.645276: step 21900, loss = 0.75, accuracy = 0.81 (975.5 examples/sec; 0.131 sec/batch)
2017-05-11 12:33:09.932900: step 22000, loss = 0.65, accuracy = 0.84 (1222.3 examples/sec; 0.105 sec/batch)
2017-05-11 12:33:21.401825: step 22100, loss = 0.59, accuracy = 0.88 (1066.0 examples/sec; 0.120 sec/batch)
2017-05-11 12:33:32.815386: step 22200, loss = 0.85, accuracy = 0.75 (1045.1 examples/sec; 0.122 sec/batch)
2017-05-11 12:33:44.245574: step 22300, loss = 0.70, accuracy = 0.79 (1055.0 examples/sec; 0.121 sec/batch)
2017-05-11 12:33:55.701250: step 22400, loss = 0.87, accuracy = 0.76 (1075.5 examples/sec; 0.119 sec/batch)
2017-05-11 12:34:06.830600: step 22500, loss = 0.79, accuracy = 0.76 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-11 12:34:18.241318: step 22600, loss = 0.73, accuracy = 0.80 (981.0 examples/sec; 0.130 sec/batch)
2017-05-11 12:34:29.379695: step 22700, loss = 0.76, accuracy = 0.81 (1191.0 examples/sec; 0.107 sec/batch)
2017-05-11 12:34:40.617048: step 22800, loss = 0.79, accuracy = 0.80 (1023.8 examples/sec; 0.125 sec/batch)
2017-05-11 12:34:52.016746: step 22900, loss = 0.74, accuracy = 0.78 (1124.7 examples/sec; 0.114 sec/batch)
2017-05-11 12:35:03.172875: step 23000, loss = 0.76, accuracy = 0.81 (1178.6 examples/sec; 0.109 sec/batch)
2017-05-11 12:35:14.499900: step 23100, loss = 0.78, accuracy = 0.80 (1109.7 examples/sec; 0.115 sec/batch)
2017-05-11 12:35:25.882584: step 23200, loss = 0.70, accuracy = 0.82 (1063.1 examples/sec; 0.120 sec/batch)
2017-05-11 12:35:37.139688: step 23300, loss = 0.73, accuracy = 0.82 (1249.7 examples/sec; 0.102 sec/batch)
2017-05-11 12:35:48.434213: step 23400, loss = 0.70, accuracy = 0.81 (1173.5 examples/sec; 0.109 sec/batch)
2017-05-11 12:35:59.720143: step 23500, loss = 0.59, accuracy = 0.84 (1146.7 examples/sec; 0.112 sec/batch)
2017-05-11 12:36:11.123931: step 23600, loss = 0.96, accuracy = 0.68 (905.9 examples/sec; 0.141 sec/batch)
2017-05-11 12:36:22.148434: step 23700, loss = 0.63, accuracy = 0.86 (1022.7 examples/sec; 0.125 sec/batch)
2017-05-11 12:36:33.405637: step 23800, loss = 0.67, accuracy = 0.84 (1147.8 examples/sec; 0.112 sec/batch)
2017-05-11 12:36:44.729174: step 23900, loss = 0.67, accuracy = 0.83 (1215.8 examples/sec; 0.105 sec/batch)
2017-05-11 12:36:55.926460: step 24000, loss = 0.89, accuracy = 0.77 (1300.7 examples/sec; 0.098 sec/batch)
2017-05-11 12:37:07.021032: step 24100, loss = 0.62, accuracy = 0.87 (1120.4 examples/sec; 0.114 sec/batch)
2017-05-11 12:37:18.208354: step 24200, loss = 0.79, accuracy = 0.81 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-11 12:37:29.695854: step 24300, loss = 0.74, accuracy = 0.77 (1044.7 examples/sec; 0.123 sec/batch)
2017-05-11 12:37:41.216197: step 24400, loss = 0.64, accuracy = 0.84 (1044.6 examples/sec; 0.123 sec/batch)
2017-05-11 12:37:52.528652: step 24500, loss = 0.82, accuracy = 0.77 (1110.0 examples/sec; 0.115 sec/batch)
2017-05-11 12:38:03.934753: step 24600, loss = 0.72, accuracy = 0.84 (1146.2 examples/sec; 0.112 sec/batch)
2017-05-11 12:38:15.361746: step 24700, loss = 0.60, accuracy = 0.85 (1220.7 examples/sec; 0.105 sec/batch)
2017-05-11 12:38:26.588034: step 24800, loss = 0.80, accuracy = 0.79 (1190.7 examples/sec; 0.107 sec/batch)
2017-05-11 12:38:38.067156: step 24900, loss = 0.64, accuracy = 0.82 (940.2 examples/sec; 0.136 sec/batch)
2017-05-11 12:38:49.406028: step 25000, loss = 0.59, accuracy = 0.88 (1081.7 examples/sec; 0.118 sec/batch)
2017-05-11 12:39:00.542008: step 25100, loss = 0.74, accuracy = 0.76 (1086.2 examples/sec; 0.118 sec/batch)
2017-05-11 12:39:11.934960: step 25200, loss = 0.72, accuracy = 0.80 (1103.4 examples/sec; 0.116 sec/batch)
2017-05-11 12:39:23.327778: step 25300, loss = 0.68, accuracy = 0.83 (1328.1 examples/sec; 0.096 sec/batch)
2017-05-11 12:39:34.799547: step 25400, loss = 0.55, accuracy = 0.86 (1107.3 examples/sec; 0.116 sec/batch)
2017-05-11 12:39:46.276889: step 25500, loss = 0.75, accuracy = 0.85 (1098.9 examples/sec; 0.116 sec/batch)
2017-05-11 12:39:57.560287: step 25600, loss = 0.87, accuracy = 0.74 (935.9 examples/sec; 0.137 sec/batch)
2017-05-11 12:40:09.037519: step 25700, loss = 0.66, accuracy = 0.84 (1261.4 examples/sec; 0.101 sec/batch)
2017-05-11 12:40:20.239374: step 25800, loss = 0.64, accuracy = 0.84 (991.6 examples/sec; 0.129 sec/batch)
2017-05-11 12:40:31.658077: step 25900, loss = 0.62, accuracy = 0.84 (1142.2 examples/sec; 0.112 sec/batch)
2017-05-11 12:40:42.907884: step 26000, loss = 0.76, accuracy = 0.80 (1097.0 examples/sec; 0.117 sec/batch)
2017-05-11 12:40:54.313974: step 26100, loss = 0.69, accuracy = 0.84 (1233.6 examples/sec; 0.104 sec/batch)
2017-05-11 12:41:05.425872: step 26200, loss = 0.74, accuracy = 0.80 (1101.1 examples/sec; 0.116 sec/batch)
2017-05-11 12:41:16.615185: step 26300, loss = 0.74, accuracy = 0.80 (1040.8 examples/sec; 0.123 sec/batch)
2017-05-11 12:41:28.018735: step 26400, loss = 0.70, accuracy = 0.85 (1297.1 examples/sec; 0.099 sec/batch)
2017-05-11 12:41:39.212184: step 26500, loss = 0.76, accuracy = 0.77 (1230.0 examples/sec; 0.104 sec/batch)
2017-05-11 12:41:50.539150: step 26600, loss = 0.80, accuracy = 0.80 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-11 12:42:01.907084: step 26700, loss = 0.67, accuracy = 0.84 (1062.3 examples/sec; 0.120 sec/batch)
2017-05-11 12:42:12.977035: step 26800, loss = 0.76, accuracy = 0.82 (1120.1 examples/sec; 0.114 sec/batch)
2017-05-11 12:42:24.259567: step 26900, loss = 0.77, accuracy = 0.80 (1109.6 examples/sec; 0.115 sec/batch)
2017-05-11 12:42:35.449737: step 27000, loss = 0.85, accuracy = 0.78 (1185.3 examples/sec; 0.108 sec/batch)
2017-05-11 12:42:46.813656: step 27100, loss = 0.78, accuracy = 0.78 (971.6 examples/sec; 0.132 sec/batch)
2017-05-11 12:42:57.731340: step 27200, loss = 0.83, accuracy = 0.75 (1143.8 examples/sec; 0.112 sec/batch)
2017-05-11 12:43:09.099853: step 27300, loss = 0.79, accuracy = 0.80 (998.1 examples/sec; 0.128 sec/batch)
2017-05-11 12:43:20.365375: step 27400, loss = 0.71, accuracy = 0.82 (1159.6 examples/sec; 0.110 sec/batch)
2017-05-11 12:43:31.632338: step 27500, loss = 0.84, accuracy = 0.77 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-11 12:43:42.719247: step 27600, loss = 0.76, accuracy = 0.80 (1240.3 examples/sec; 0.103 sec/batch)
2017-05-11 12:43:53.952254: step 27700, loss = 0.74, accuracy = 0.81 (1154.2 examples/sec; 0.111 sec/batch)
2017-05-11 12:44:05.357143: step 27800, loss = 0.60, accuracy = 0.87 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-11 12:44:16.604910: step 27900, loss = 0.68, accuracy = 0.84 (928.4 examples/sec; 0.138 sec/batch)
2017-05-11 12:44:27.940854: step 28000, loss = 0.71, accuracy = 0.80 (999.1 examples/sec; 0.128 sec/batch)
2017-05-11 12:44:39.214058: step 28100, loss = 0.70, accuracy = 0.81 (1185.2 examples/sec; 0.108 sec/batch)
2017-05-11 12:44:50.447260: step 28200, loss = 0.68, accuracy = 0.82 (981.5 examples/sec; 0.130 sec/batch)
2017-05-11 12:45:01.473816: step 28300, loss = 0.65, accuracy = 0.86 (955.6 examples/sec; 0.134 sec/batch)
2017-05-11 12:45:12.491722: step 28400, loss = 0.66, accuracy = 0.85 (1136.3 examples/sec; 0.113 sec/batch)
2017-05-11 12:45:23.513781: step 28500, loss = 0.69, accuracy = 0.80 (1271.3 examples/sec; 0.101 sec/batch)
2017-05-11 12:45:34.531893: step 28600, loss = 0.64, accuracy = 0.85 (1084.1 examples/sec; 0.118 sec/batch)
2017-05-11 12:45:45.783647: step 28700, loss = 0.71, accuracy = 0.83 (1077.7 examples/sec; 0.119 sec/batch)
2017-05-11 12:45:56.806957: step 28800, loss = 0.72, accuracy = 0.82 (1015.1 examples/sec; 0.126 sec/batch)
2017-05-11 12:46:08.156367: step 28900, loss = 0.90, accuracy = 0.73 (1158.5 examples/sec; 0.110 sec/batch)
2017-05-11 12:46:19.412314: step 29000, loss = 0.76, accuracy = 0.80 (1225.0 examples/sec; 0.104 sec/batch)
2017-05-11 12:46:30.613159: step 29100, loss = 0.58, accuracy = 0.87 (967.2 examples/sec; 0.132 sec/batch)
2017-05-11 12:46:41.824521: step 29200, loss = 0.68, accuracy = 0.83 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-11 12:46:53.162544: step 29300, loss = 0.54, accuracy = 0.88 (1302.4 examples/sec; 0.098 sec/batch)
2017-05-11 12:47:04.181775: step 29400, loss = 0.63, accuracy = 0.86 (1155.4 examples/sec; 0.111 sec/batch)
2017-05-11 12:47:15.209343: step 29500, loss = 0.59, accuracy = 0.82 (927.6 examples/sec; 0.138 sec/batch)
2017-05-11 12:47:26.428558: step 29600, loss = 0.72, accuracy = 0.77 (945.9 examples/sec; 0.135 sec/batch)
2017-05-11 12:47:37.682926: step 29700, loss = 0.70, accuracy = 0.80 (1051.6 examples/sec; 0.122 sec/batch)
2017-05-11 12:47:48.544334: step 29800, loss = 0.59, accuracy = 0.88 (1154.3 examples/sec; 0.111 sec/batch)
2017-05-11 12:47:59.807075: step 29900, loss = 0.60, accuracy = 0.87 (959.4 examples/sec; 0.133 sec/batch)
2017-05-11 12:48:10.948980: step 30000, loss = 0.59, accuracy = 0.90 (1181.0 examples/sec; 0.108 sec/batch)
2017-05-11 12:48:22.159359: step 30100, loss = 0.63, accuracy = 0.85 (1049.3 examples/sec; 0.122 sec/batch)
2017-05-11 12:48:33.253821: step 30200, loss = 0.76, accuracy = 0.80 (1070.5 examples/sec; 0.120 sec/batch)
2017-05-11 12:48:44.436720: step 30300, loss = 0.88, accuracy = 0.74 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-11 12:48:55.623860: step 30400, loss = 0.64, accuracy = 0.84 (1194.3 examples/sec; 0.107 sec/batch)
2017-05-11 12:49:06.888586: step 30500, loss = 0.67, accuracy = 0.81 (1071.3 examples/sec; 0.119 sec/batch)
2017-05-11 12:49:18.413703: step 30600, loss = 0.83, accuracy = 0.80 (992.4 examples/sec; 0.129 sec/batch)
2017-05-11 12:49:29.488374: step 30700, loss = 0.71, accuracy = 0.83 (1262.2 examples/sec; 0.101 sec/batch)
2017-05-11 12:49:40.811756: step 30800, loss = 0.84, accuracy = 0.78 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-11 12:49:51.928787: step 30900, loss = 0.81, accuracy = 0.78 (1030.9 examples/sec; 0.124 sec/batch)
2017-05-11 12:50:03.300367: step 31000, loss = 0.78, accuracy = 0.81 (1141.8 examples/sec; 0.112 sec/batch)
2017-05-11 12:50:14.222158: step 31100, loss = 0.77, accuracy = 0.79 (1083.9 examples/sec; 0.118 sec/batch)
2017-05-11 12:50:25.503974: step 31200, loss = 0.91, accuracy = 0.79 (945.5 examples/sec; 0.135 sec/batch)
2017-05-11 12:50:36.834588: step 31300, loss = 0.78, accuracy = 0.77 (985.5 examples/sec; 0.130 sec/batch)
2017-05-11 12:50:47.975529: step 31400, loss = 0.79, accuracy = 0.79 (1285.8 examples/sec; 0.100 sec/batch)
2017-05-11 12:50:58.964583: step 31500, loss = 0.70, accuracy = 0.83 (1188.8 examples/sec; 0.108 sec/batch)
2017-05-11 12:51:10.122649: step 31600, loss = 0.65, accuracy = 0.84 (1160.2 examples/sec; 0.110 sec/batch)
2017-05-11 12:51:21.129893: step 31700, loss = 0.72, accuracy = 0.84 (1214.2 examples/sec; 0.105 sec/batch)
2017-05-11 12:51:32.209909: step 31800, loss = 0.65, accuracy = 0.84 (925.9 examples/sec; 0.138 sec/batch)
2017-05-11 12:51:43.427714: step 31900, loss = 0.70, accuracy = 0.83 (1196.0 examples/sec; 0.107 sec/batch)
2017-05-11 12:51:55.005611: step 32000, loss = 0.71, accuracy = 0.80 (970.4 examples/sec; 0.132 sec/batch)
2017-05-11 12:52:06.354144: step 32100, loss = 0.74, accuracy = 0.84 (1036.9 examples/sec; 0.123 sec/batch)
2017-05-11 12:52:17.531238: step 32200, loss = 0.67, accuracy = 0.84 (1241.1 examples/sec; 0.103 sec/batch)
2017-05-11 12:52:28.807831: step 32300, loss = 0.71, accuracy = 0.83 (1100.2 examples/sec; 0.116 sec/batch)
2017-05-11 12:52:40.123941: step 32400, loss = 0.76, accuracy = 0.79 (1145.6 examples/sec; 0.112 sec/batch)
2017-05-11 12:52:51.525315: step 32500, loss = 0.72, accuracy = 0.84 (1177.0 examples/sec; 0.109 sec/batch)
2017-05-11 12:53:02.779933: step 32600, loss = 0.62, accuracy = 0.83 (1127.4 examples/sec; 0.114 sec/batch)
2017-05-11 12:53:14.079717: step 32700, loss = 0.57, accuracy = 0.84 (1000.2 examples/sec; 0.128 sec/batch)
2017-05-11 12:53:25.548177: step 32800, loss = 0.65, accuracy = 0.84 (960.4 examples/sec; 0.133 sec/batch)
2017-05-11 12:53:36.768553: step 32900, loss = 0.70, accuracy = 0.87 (1165.2 examples/sec; 0.110 sec/batch)
2017-05-11 12:53:47.863684: step 33000, loss = 0.65, accuracy = 0.87 (963.1 examples/sec; 0.133 sec/batch)
2017-05-11 12:53:59.207409: step 33100, loss = 0.77, accuracy = 0.80 (984.7 examples/sec; 0.130 sec/batch)
2017-05-11 12:54:10.478557: step 33200, loss = 0.74, accuracy = 0.80 (993.2 examples/sec; 0.129 sec/batch)
2017-05-11 12:54:21.842426: step 33300, loss = 0.64, accuracy = 0.83 (1267.7 examples/sec; 0.101 sec/batch)
2017-05-11 12:54:33.384469: step 33400, loss = 0.78, accuracy = 0.80 (1103.1 examples/sec; 0.116 sec/batch)
2017-05-11 12:54:44.774773: step 33500, loss = 0.66, accuracy = 0.89 (1107.2 examples/sec; 0.116 sec/batch)
2017-05-11 12:54:56.145844: step 33600, loss = 0.64, accuracy = 0.85 (901.2 examples/sec; 0.142 sec/batch)
2017-05-11 12:55:07.616074: step 33700, loss = 0.82, accuracy = 0.78 (1042.8 examples/sec; 0.123 sec/batch)
2017-05-11 12:55:18.875769: step 33800, loss = 0.56, accuracy = 0.86 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-11 12:55:30.175594: step 33900, loss = 0.70, accuracy = 0.80 (1111.7 examples/sec; 0.115 sec/batch)
2017-05-11 12:55:41.427600: step 34000, loss = 0.73, accuracy = 0.83 (1236.9 examples/sec; 0.103 sec/batch)
2017-05-11 12:55:52.541791: step 34100, loss = 0.66, accuracy = 0.84 (1156.7 examples/sec; 0.111 sec/batch)
2017-05-11 12:56:03.594296: step 34200, loss = 0.76, accuracy = 0.79 (1240.9 examples/sec; 0.103 sec/batch)
2017-05-11 12:56:14.740455: step 34300, loss = 0.62, accuracy = 0.87 (1346.3 examples/sec; 0.095 sec/batch)
2017-05-11 12:56:26.196822: step 34400, loss = 0.65, accuracy = 0.81 (1006.2 examples/sec; 0.127 sec/batch)
2017-05-11 12:56:37.381398: step 34500, loss = 0.67, accuracy = 0.80 (1346.6 examples/sec; 0.095 sec/batch)
2017-05-11 12:56:48.722271: step 34600, loss = 0.68, accuracy = 0.83 (1228.0 examples/sec; 0.104 sec/batch)
2017-05-11 12:56:59.891534: step 34700, loss = 0.81, accuracy = 0.77 (1188.1 examples/sec; 0.108 sec/batch)
2017-05-11 12:57:10.849767: step 34800, loss = 0.76, accuracy = 0.82 (1132.4 examples/sec; 0.113 sec/batch)
2017-05-11 12:57:22.333218: step 34900, loss = 0.71, accuracy = 0.84 (1138.9 examples/sec; 0.112 sec/batch)
2017-05-11 12:57:33.649218: step 35000, loss = 0.68, accuracy = 0.81 (1049.0 examples/sec; 0.122 sec/batch)
2017-05-11 12:57:45.123084: step 35100, loss = 0.74, accuracy = 0.82 (1120.4 examples/sec; 0.114 sec/batch)
2017-05-11 12:57:56.296589: step 35200, loss = 0.73, accuracy = 0.82 (1019.6 examples/sec; 0.126 sec/batch)
2017-05-11 12:58:07.446049: step 35300, loss = 0.65, accuracy = 0.87 (1108.9 examples/sec; 0.115 sec/batch)
2017-05-11 12:58:18.803026: step 35400, loss = 0.80, accuracy = 0.76 (1134.4 examples/sec; 0.113 sec/batch)
2017-05-11 12:58:30.062234: step 35500, loss = 0.73, accuracy = 0.83 (1139.9 examples/sec; 0.112 sec/batch)
2017-05-11 12:58:41.277803: step 35600, loss = 0.65, accuracy = 0.84 (1085.0 examples/sec; 0.118 sec/batch)
2017-05-11 12:58:52.402656: step 35700, loss = 0.80, accuracy = 0.77 (1020.6 examples/sec; 0.125 sec/batch)
2017-05-11 12:59:03.837403: step 35800, loss = 0.66, accuracy = 0.80 (1089.5 examples/sec; 0.117 sec/batch)
2017-05-11 12:59:14.913891: step 35900, loss = 0.94, accuracy = 0.77 (1132.7 examples/sec; 0.113 sec/batch)
2017-05-11 12:59:26.238410: step 36000, loss = 0.67, accuracy = 0.84 (1127.6 examples/sec; 0.114 sec/batch)
2017-05-11 12:59:37.413340: step 36100, loss = 0.82, accuracy = 0.77 (1162.9 examples/sec; 0.110 sec/batch)
2017-05-11 12:59:48.741732: step 36200, loss = 0.68, accuracy = 0.86 (1031.0 examples/sec; 0.124 sec/batch)
2017-05-11 12:59:59.919965: step 36300, loss = 0.70, accuracy = 0.83 (1199.8 examples/sec; 0.107 sec/batch)
2017-05-11 13:00:11.066860: step 36400, loss = 0.68, accuracy = 0.84 (1056.1 examples/sec; 0.121 sec/batch)
2017-05-11 13:00:22.162718: step 36500, loss = 0.64, accuracy = 0.84 (1114.6 examples/sec; 0.115 sec/batch)
2017-05-11 13:00:33.555849: step 36600, loss = 0.64, accuracy = 0.85 (969.0 examples/sec; 0.132 sec/batch)
2017-05-11 13:00:45.260332: step 36700, loss = 0.78, accuracy = 0.81 (1052.1 examples/sec; 0.122 sec/batch)
2017-05-11 13:00:56.402296: step 36800, loss = 0.78, accuracy = 0.81 (1102.5 examples/sec; 0.116 sec/batch)
2017-05-11 13:01:07.960672: step 36900, loss = 0.62, accuracy = 0.84 (1176.4 examples/sec; 0.109 sec/batch)
2017-05-11 13:01:19.400619: step 37000, loss = 0.84, accuracy = 0.76 (1072.1 examples/sec; 0.119 sec/batch)
2017-05-11 13:01:30.703582: step 37100, loss = 0.69, accuracy = 0.84 (1230.2 examples/sec; 0.104 sec/batch)
2017-05-11 13:01:41.874363: step 37200, loss = 0.56, accuracy = 0.88 (1137.3 examples/sec; 0.113 sec/batch)
2017-05-11 13:01:53.410497: step 37300, loss = 0.66, accuracy = 0.83 (951.3 examples/sec; 0.135 sec/batch)
2017-05-11 13:02:04.703136: step 37400, loss = 0.63, accuracy = 0.85 (1105.3 examples/sec; 0.116 sec/batch)
2017-05-11 13:02:16.084504: step 37500, loss = 0.70, accuracy = 0.82 (1062.0 examples/sec; 0.121 sec/batch)
2017-05-11 13:02:27.357166: step 37600, loss = 0.63, accuracy = 0.84 (1172.3 examples/sec; 0.109 sec/batch)
2017-05-11 13:02:38.788933: step 37700, loss = 0.65, accuracy = 0.82 (930.9 examples/sec; 0.138 sec/batch)
2017-05-11 13:02:50.404896: step 37800, loss = 0.58, accuracy = 0.89 (984.0 examples/sec; 0.130 sec/batch)
2017-05-11 13:03:01.967624: step 37900, loss = 0.64, accuracy = 0.91 (993.5 examples/sec; 0.129 sec/batch)
2017-05-11 13:03:13.168442: step 38000, loss = 0.81, accuracy = 0.79 (1075.8 examples/sec; 0.119 sec/batch)
2017-05-11 13:03:24.864197: step 38100, loss = 0.61, accuracy = 0.84 (969.7 examples/sec; 0.132 sec/batch)
2017-05-11 13:03:36.132262: step 38200, loss = 0.81, accuracy = 0.80 (1026.6 examples/sec; 0.125 sec/batch)
2017-05-11 13:03:47.585295: step 38300, loss = 0.64, accuracy = 0.86 (1017.8 examples/sec; 0.126 sec/batch)
2017-05-11 13:03:59.051996: step 38400, loss = 0.70, accuracy = 0.88 (940.7 examples/sec; 0.136 sec/batch)
2017-05-11 13:04:10.462639: step 38500, loss = 0.56, accuracy = 0.88 (1119.7 examples/sec; 0.114 sec/batch)
2017-05-11 13:04:21.975865: step 38600, loss = 0.62, accuracy = 0.86 (1110.5 examples/sec; 0.115 sec/batch)
2017-05-11 13:04:33.351735: step 38700, loss = 0.66, accuracy = 0.88 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-11 13:04:44.780441: step 38800, loss = 0.65, accuracy = 0.88 (1144.2 examples/sec; 0.112 sec/batch)
2017-05-11 13:04:55.934014: step 38900, loss = 0.77, accuracy = 0.80 (1002.2 examples/sec; 0.128 sec/batch)
2017-05-11 13:05:07.268933: step 39000, loss = 0.61, accuracy = 0.86 (983.3 examples/sec; 0.130 sec/batch)
2017-05-11 13:05:18.574161: step 39100, loss = 0.64, accuracy = 0.86 (1096.5 examples/sec; 0.117 sec/batch)
2017-05-11 13:05:29.767372: step 39200, loss = 0.64, accuracy = 0.88 (1140.8 examples/sec; 0.112 sec/batch)
2017-05-11 13:05:40.714964: step 39300, loss = 0.75, accuracy = 0.81 (1074.7 examples/sec; 0.119 sec/batch)
2017-05-11 13:05:51.960022: step 39400, loss = 0.73, accuracy = 0.78 (1044.9 examples/sec; 0.123 sec/batch)
2017-05-11 13:06:03.255393: step 39500, loss = 0.66, accuracy = 0.86 (1094.3 examples/sec; 0.117 sec/batch)
2017-05-11 13:06:14.569593: step 39600, loss = 0.74, accuracy = 0.80 (1018.8 examples/sec; 0.126 sec/batch)
2017-05-11 13:06:25.778199: step 39700, loss = 0.72, accuracy = 0.80 (952.6 examples/sec; 0.134 sec/batch)
2017-05-11 13:06:37.332544: step 39800, loss = 0.90, accuracy = 0.77 (1041.7 examples/sec; 0.123 sec/batch)
2017-05-11 13:06:48.739630: step 39900, loss = 0.64, accuracy = 0.87 (1150.0 examples/sec; 0.111 sec/batch)
2017-05-11 13:07:00.150828: step 40000, loss = 0.81, accuracy = 0.79 (1080.3 examples/sec; 0.118 sec/batch)
2017-05-11 13:07:11.410623: step 40100, loss = 0.65, accuracy = 0.82 (1132.3 examples/sec; 0.113 sec/batch)
2017-05-11 13:07:22.773510: step 40200, loss = 0.87, accuracy = 0.74 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-11 13:07:34.199138: step 40300, loss = 0.70, accuracy = 0.85 (883.4 examples/sec; 0.145 sec/batch)
2017-05-11 13:07:45.378326: step 40400, loss = 0.62, accuracy = 0.83 (1177.9 examples/sec; 0.109 sec/batch)
2017-05-11 13:07:56.583238: step 40500, loss = 0.66, accuracy = 0.86 (1110.4 examples/sec; 0.115 sec/batch)
2017-05-11 13:08:07.696311: step 40600, loss = 0.74, accuracy = 0.80 (896.0 examples/sec; 0.143 sec/batch)
2017-05-11 13:08:19.002053: step 40700, loss = 0.77, accuracy = 0.80 (1385.8 examples/sec; 0.092 sec/batch)
2017-05-11 13:08:30.559292: step 40800, loss = 0.66, accuracy = 0.84 (1200.0 examples/sec; 0.107 sec/batch)
2017-05-11 13:08:42.037513: step 40900, loss = 0.73, accuracy = 0.84 (949.2 examples/sec; 0.135 sec/batch)
2017-05-11 13:08:53.107223: step 41000, loss = 0.82, accuracy = 0.81 (1013.8 examples/sec; 0.126 sec/batch)
2017-05-11 13:09:04.236632: step 41100, loss = 0.70, accuracy = 0.85 (1231.5 examples/sec; 0.104 sec/batch)
2017-05-11 13:09:15.673268: step 41200, loss = 0.76, accuracy = 0.77 (995.0 examples/sec; 0.129 sec/batch)
2017-05-11 13:09:26.804198: step 41300, loss = 0.67, accuracy = 0.83 (1132.4 examples/sec; 0.113 sec/batch)
2017-05-11 13:09:37.884274: step 41400, loss = 0.71, accuracy = 0.82 (1097.7 examples/sec; 0.117 sec/batch)
2017-05-11 13:09:49.390872: step 41500, loss = 0.57, accuracy = 0.89 (1038.4 examples/sec; 0.123 sec/batch)
2017-05-11 13:10:00.839405: step 41600, loss = 0.71, accuracy = 0.82 (1040.9 examples/sec; 0.123 sec/batch)
2017-05-11 13:10:12.460660: step 41700, loss = 0.79, accuracy = 0.82 (1068.4 examples/sec; 0.120 sec/batch)
2017-05-11 13:10:23.685951: step 41800, loss = 0.81, accuracy = 0.80 (984.5 examples/sec; 0.130 sec/batch)
2017-05-11 13:10:34.733811: step 41900, loss = 0.71, accuracy = 0.84 (1012.2 examples/sec; 0.126 sec/batch)
2017-05-11 13:10:45.940471: step 42000, loss = 0.79, accuracy = 0.79 (1270.9 examples/sec; 0.101 sec/batch)
2017-05-11 13:10:57.181672: step 42100, loss = 0.66, accuracy = 0.83 (996.4 examples/sec; 0.128 sec/batch)
2017-05-11 13:11:08.406516: step 42200, loss = 0.71, accuracy = 0.84 (1082.7 examples/sec; 0.118 sec/batch)
2017-05-11 13:11:19.699707: step 42300, loss = 0.74, accuracy = 0.82 (928.2 examples/sec; 0.138 sec/batch)
2017-05-11 13:11:30.784734: step 42400, loss = 0.70, accuracy = 0.81 (1036.6 examples/sec; 0.123 sec/batch)
2017-05-11 13:11:41.889480: step 42500, loss = 0.71, accuracy = 0.85 (891.0 examples/sec; 0.144 sec/batch)
2017-05-11 13:11:53.144478: step 42600, loss = 0.65, accuracy = 0.88 (1076.7 examples/sec; 0.119 sec/batch)
2017-05-11 13:12:04.344334: step 42700, loss = 0.75, accuracy = 0.83 (1079.7 examples/sec; 0.119 sec/batch)
2017-05-11 13:12:15.371096: step 42800, loss = 0.81, accuracy = 0.78 (1042.4 examples/sec; 0.123 sec/batch)
2017-05-11 13:12:26.555365: step 42900, loss = 0.76, accuracy = 0.80 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-11 13:12:37.714135: step 43000, loss = 0.69, accuracy = 0.80 (1219.9 examples/sec; 0.105 sec/batch)
2017-05-11 13:12:49.125774: step 43100, loss = 0.66, accuracy = 0.84 (894.2 examples/sec; 0.143 sec/batch)
2017-05-11 13:13:00.351061: step 43200, loss = 0.66, accuracy = 0.85 (1019.8 examples/sec; 0.126 sec/batch)
2017-05-11 13:13:11.788472: step 43300, loss = 0.66, accuracy = 0.85 (1222.3 examples/sec; 0.105 sec/batch)
2017-05-11 13:13:23.050946: step 43400, loss = 0.68, accuracy = 0.81 (1040.5 examples/sec; 0.123 sec/batch)
2017-05-11 13:13:34.278716: step 43500, loss = 0.82, accuracy = 0.75 (1255.0 examples/sec; 0.102 sec/batch)
2017-05-11 13:13:45.317973: step 43600, loss = 0.61, accuracy = 0.86 (1070.9 examples/sec; 0.120 sec/batch)
2017-05-11 13:13:56.585907: step 43700, loss = 0.67, accuracy = 0.87 (1049.2 examples/sec; 0.122 sec/batch)
2017-05-11 13:14:07.826666: step 43800, loss = 0.73, accuracy = 0.87 (1087.4 examples/sec; 0.118 sec/batch)
2017-05-11 13:14:19.199532: step 43900, loss = 0.66, accuracy = 0.84 (1129.5 examples/sec; 0.113 sec/batch)
2017-05-11 13:14:30.203432: step 44000, loss = 0.70, accuracy = 0.84 (937.3 examples/sec; 0.137 sec/batch)
2017-05-11 13:14:41.433442: step 44100, loss = 0.60, accuracy = 0.88 (1194.2 examples/sec; 0.107 sec/batch)
2017-05-11 13:14:52.502310: step 44200, loss = 0.59, accuracy = 0.89 (1038.1 examples/sec; 0.123 sec/batch)
2017-05-11 13:15:03.626263: step 44300, loss = 0.54, accuracy = 0.87 (1007.3 examples/sec; 0.127 sec/batch)
2017-05-11 13:15:14.790578: step 44400, loss = 0.70, accuracy = 0.83 (1171.3 examples/sec; 0.109 sec/batch)
2017-05-11 13:15:25.902310: step 44500, loss = 0.75, accuracy = 0.80 (851.1 examples/sec; 0.150 sec/batch)
2017-05-11 13:15:36.990243: step 44600, loss = 0.57, accuracy = 0.87 (1170.8 examples/sec; 0.109 sec/batch)
2017-05-11 13:15:48.120306: step 44700, loss = 0.72, accuracy = 0.79 (1054.7 examples/sec; 0.121 sec/batch)
2017-05-11 13:15:59.331010: step 44800, loss = 0.74, accuracy = 0.86 (1001.3 examples/sec; 0.128 sec/batch)
2017-05-11 13:16:10.637517: step 44900, loss = 0.71, accuracy = 0.81 (973.1 examples/sec; 0.132 sec/batch)
2017-05-11 13:16:21.707649: step 45000, loss = 0.61, accuracy = 0.84 (1087.6 examples/sec; 0.118 sec/batch)
2017-05-11 13:16:33.001380: step 45100, loss = 0.72, accuracy = 0.79 (1040.6 examples/sec; 0.123 sec/batch)
2017-05-11 13:16:44.328846: step 45200, loss = 0.80, accuracy = 0.79 (1053.7 examples/sec; 0.121 sec/batch)
2017-05-11 13:16:55.551707: step 45300, loss = 0.75, accuracy = 0.82 (1102.7 examples/sec; 0.116 sec/batch)
2017-05-11 13:17:06.697921: step 45400, loss = 0.73, accuracy = 0.80 (1105.1 examples/sec; 0.116 sec/batch)
2017-05-11 13:17:17.923036: step 45500, loss = 0.74, accuracy = 0.82 (1127.1 examples/sec; 0.114 sec/batch)
2017-05-11 13:17:29.033607: step 45600, loss = 0.80, accuracy = 0.80 (1120.1 examples/sec; 0.114 sec/batch)
2017-05-11 13:17:40.228818: step 45700, loss = 0.79, accuracy = 0.83 (1207.1 examples/sec; 0.106 sec/batch)
2017-05-11 13:17:51.593536: step 45800, loss = 0.81, accuracy = 0.80 (1198.8 examples/sec; 0.107 sec/batch)
2017-05-11 13:18:02.705818: step 45900, loss = 0.68, accuracy = 0.82 (1112.3 examples/sec; 0.115 sec/batch)
2017-05-11 13:18:14.004706: step 46000, loss = 0.64, accuracy = 0.84 (1127.9 examples/sec; 0.113 sec/batch)
2017-05-11 13:18:25.269035: step 46100, loss = 0.63, accuracy = 0.86 (1020.3 examples/sec; 0.125 sec/batch)
2017-05-11 13:18:36.415144: step 46200, loss = 0.68, accuracy = 0.82 (1127.4 examples/sec; 0.114 sec/batch)
2017-05-11 13:18:47.656038: step 46300, loss = 0.55, accuracy = 0.87 (942.6 examples/sec; 0.136 sec/batch)
2017-05-11 13:18:59.104270: step 46400, loss = 0.70, accuracy = 0.80 (879.3 examples/sec; 0.146 sec/batch)
2017-05-11 13:19:10.266223: step 46500, loss = 0.74, accuracy = 0.81 (999.0 examples/sec; 0.128 sec/batch)
2017-05-11 13:19:21.489609: step 46600, loss = 0.73, accuracy = 0.82 (1189.4 examples/sec; 0.108 sec/batch)
2017-05-11 13:19:32.928107: step 46700, loss = 0.67, accuracy = 0.83 (1156.2 examples/sec; 0.111 sec/batch)
2017-05-11 13:19:44.159697: step 46800, loss = 0.66, accuracy = 0.84 (1240.5 examples/sec; 0.103 sec/batch)
2017-05-11 13:19:55.540503: step 46900, loss = 0.61, accuracy = 0.84 (1062.8 examples/sec; 0.120 sec/batch)
2017-05-11 13:20:06.755144: step 47000, loss = 0.68, accuracy = 0.80 (1119.7 examples/sec; 0.114 sec/batch)
2017-05-11 13:20:17.954710: step 47100, loss = 0.63, accuracy = 0.86 (889.9 examples/sec; 0.144 sec/batch)
2017-05-11 13:20:29.243616: step 47200, loss = 0.57, accuracy = 0.90 (1034.2 examples/sec; 0.124 sec/batch)
2017-05-11 13:20:40.492888: step 47300, loss = 0.55, accuracy = 0.90 (1077.5 examples/sec; 0.119 sec/batch)
2017-05-11 13:20:51.756887: step 47400, loss = 0.68, accuracy = 0.85 (1023.0 examples/sec; 0.125 sec/batch)
2017-05-11 13:21:02.992893: step 47500, loss = 0.76, accuracy = 0.82 (1027.6 examples/sec; 0.125 sec/batch)
2017-05-11 13:21:14.395964: step 47600, loss = 0.55, accuracy = 0.88 (958.4 examples/sec; 0.134 sec/batch)
2017-05-11 13:21:25.545248: step 47700, loss = 0.58, accuracy = 0.88 (1098.8 examples/sec; 0.116 sec/batch)
2017-05-11 13:21:36.716129: step 47800, loss = 0.72, accuracy = 0.80 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-11 13:21:47.738049: step 47900, loss = 0.75, accuracy = 0.77 (1322.3 examples/sec; 0.097 sec/batch)
2017-05-11 13:21:59.132526: step 48000, loss = 0.62, accuracy = 0.88 (1024.7 examples/sec; 0.125 sec/batch)
2017-05-11 13:22:10.292778: step 48100, loss = 0.79, accuracy = 0.78 (1208.4 examples/sec; 0.106 sec/batch)
2017-05-11 13:22:21.540242: step 48200, loss = 0.67, accuracy = 0.80 (1121.0 examples/sec; 0.114 sec/batch)
2017-05-11 13:22:32.725300: step 48300, loss = 0.59, accuracy = 0.87 (979.4 examples/sec; 0.131 sec/batch)
2017-05-11 13:22:44.014005: step 48400, loss = 0.61, accuracy = 0.84 (1213.3 examples/sec; 0.105 sec/batch)
2017-05-11 13:22:55.334281: step 48500, loss = 0.64, accuracy = 0.86 (1154.2 examples/sec; 0.111 sec/batch)
2017-05-11 13:23:06.469644: step 48600, loss = 0.76, accuracy = 0.80 (1099.8 examples/sec; 0.116 sec/batch)
2017-05-11 13:23:17.893005: step 48700, loss = 0.60, accuracy = 0.87 (977.9 examples/sec; 0.131 sec/batch)
2017-05-11 13:23:29.221316: step 48800, loss = 0.63, accuracy = 0.87 (994.6 examples/sec; 0.129 sec/batch)
2017-05-11 13:23:40.616993: step 48900, loss = 0.79, accuracy = 0.80 (1077.1 examples/sec; 0.119 sec/batch)
2017-05-11 13:23:51.677164: step 49000, loss = 0.63, accuracy = 0.88 (1222.8 examples/sec; 0.105 sec/batch)
2017-05-11 13:24:02.528335: step 49100, loss = 0.60, accuracy = 0.86 (1147.0 examples/sec; 0.112 sec/batch)
2017-05-11 13:24:13.652011: step 49200, loss = 0.56, accuracy = 0.88 (1141.7 examples/sec; 0.112 sec/batch)
2017-05-11 13:24:24.465039: step 49300, loss = 0.66, accuracy = 0.86 (1031.4 examples/sec; 0.124 sec/batch)
2017-05-11 13:24:35.344918: step 49400, loss = 0.70, accuracy = 0.80 (1001.9 examples/sec; 0.128 sec/batch)
2017-05-11 13:24:46.440854: step 49500, loss = 0.73, accuracy = 0.84 (1256.9 examples/sec; 0.102 sec/batch)
2017-05-11 13:24:57.735140: step 49600, loss = 0.61, accuracy = 0.86 (1179.0 examples/sec; 0.109 sec/batch)
2017-05-11 13:25:08.880978: step 49700, loss = 0.85, accuracy = 0.75 (1065.8 examples/sec; 0.120 sec/batch)
2017-05-11 13:25:19.709426: step 49800, loss = 0.58, accuracy = 0.86 (1412.3 examples/sec; 0.091 sec/batch)
2017-05-11 13:25:30.962769: step 49900, loss = 0.74, accuracy = 0.84 (1264.5 examples/sec; 0.101 sec/batch)
2017-05-11 13:25:41.812043: step 50000, loss = 0.80, accuracy = 0.79 (1082.9 examples/sec; 0.118 sec/batch)
2017-05-11 13:25:53.007896: step 50100, loss = 0.69, accuracy = 0.84 (1002.0 examples/sec; 0.128 sec/batch)
2017-05-11 13:26:04.105370: step 50200, loss = 0.72, accuracy = 0.82 (941.8 examples/sec; 0.136 sec/batch)
2017-05-11 13:26:15.008203: step 50300, loss = 0.80, accuracy = 0.81 (1157.4 examples/sec; 0.111 sec/batch)
2017-05-11 13:26:26.101743: step 50400, loss = 0.79, accuracy = 0.81 (1147.7 examples/sec; 0.112 sec/batch)
2017-05-11 13:26:37.167543: step 50500, loss = 0.68, accuracy = 0.84 (1157.7 examples/sec; 0.111 sec/batch)
2017-05-11 13:26:48.486534: step 50600, loss = 0.69, accuracy = 0.84 (889.8 examples/sec; 0.144 sec/batch)
2017-05-11 13:26:59.689036: step 50700, loss = 0.58, accuracy = 0.91 (1090.2 examples/sec; 0.117 sec/batch)
2017-05-11 13:27:10.876718: step 50800, loss = 0.48, accuracy = 0.88 (1048.6 examples/sec; 0.122 sec/batch)
2017-05-11 13:27:22.155288: step 50900, loss = 0.79, accuracy = 0.82 (1085.8 examples/sec; 0.118 sec/batch)
2017-05-11 13:27:33.280575: step 51000, loss = 0.87, accuracy = 0.79 (1094.4 examples/sec; 0.117 sec/batch)
2017-05-11 13:27:44.336794: step 51100, loss = 0.68, accuracy = 0.84 (1081.5 examples/sec; 0.118 sec/batch)
2017-05-11 13:27:55.520503: step 51200, loss = 0.67, accuracy = 0.87 (1048.8 examples/sec; 0.122 sec/batch)
2017-05-11 13:28:06.582799: step 51300, loss = 0.70, accuracy = 0.81 (1192.6 examples/sec; 0.107 sec/batch)
2017-05-11 13:28:17.683997: step 51400, loss = 0.74, accuracy = 0.84 (1248.8 examples/sec; 0.102 sec/batch)
2017-05-11 13:28:28.979409: step 51500, loss = 0.68, accuracy = 0.78 (1093.7 examples/sec; 0.117 sec/batch)
2017-05-11 13:28:40.092865: step 51600, loss = 0.65, accuracy = 0.88 (1270.9 examples/sec; 0.101 sec/batch)
2017-05-11 13:28:51.114743: step 51700, loss = 0.69, accuracy = 0.87 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-11 13:29:02.132549: step 51800, loss = 0.73, accuracy = 0.84 (1171.1 examples/sec; 0.109 sec/batch)
2017-05-11 13:29:13.328162: step 51900, loss = 0.70, accuracy = 0.80 (1085.6 examples/sec; 0.118 sec/batch)
2017-05-11 13:29:24.455834: step 52000, loss = 0.61, accuracy = 0.84 (994.1 examples/sec; 0.129 sec/batch)
2017-05-11 13:29:35.783997: step 52100, loss = 0.69, accuracy = 0.84 (1047.1 examples/sec; 0.122 sec/batch)
2017-05-11 13:29:47.085494: step 52200, loss = 0.64, accuracy = 0.84 (1013.5 examples/sec; 0.126 sec/batch)
2017-05-11 13:29:58.371655: step 52300, loss = 0.62, accuracy = 0.87 (1233.1 examples/sec; 0.104 sec/batch)
2017-05-11 13:30:09.689004: step 52400, loss = 0.67, accuracy = 0.82 (920.1 examples/sec; 0.139 sec/batch)
2017-05-11 13:30:20.667900: step 52500, loss = 0.64, accuracy = 0.87 (1045.2 examples/sec; 0.122 sec/batch)
2017-05-11 13:30:32.110384: step 52600, loss = 0.53, accuracy = 0.90 (1113.0 examples/sec; 0.115 sec/batch)
2017-05-11 13:30:43.446749: step 52700, loss = 0.59, accuracy = 0.82 (1035.8 examples/sec; 0.124 sec/batch)
2017-05-11 13:30:54.460976: step 52800, loss = 0.84, accuracy = 0.80 (961.3 examples/sec; 0.133 sec/batch)
2017-05-11 13:31:05.870127: step 52900, loss = 0.59, accuracy = 0.87 (1162.8 examples/sec; 0.110 sec/batch)
2017-05-11 13:31:16.948050: step 53000, loss = 0.62, accuracy = 0.81 (1052.0 examples/sec; 0.122 sec/batch)
2017-05-11 13:31:28.126489: step 53100, loss = 0.67, accuracy = 0.82 (1202.5 examples/sec; 0.106 sec/batch)
2017-05-11 13:31:39.351637: step 53200, loss = 0.69, accuracy = 0.80 (875.9 examples/sec; 0.146 sec/batch)
2017-05-11 13:31:50.473912: step 53300, loss = 0.72, accuracy = 0.80 (1108.0 examples/sec; 0.116 sec/batch)
2017-05-11 13:32:01.719676: step 53400, loss = 0.76, accuracy = 0.78 (1090.9 examples/sec; 0.117 sec/batch)
2017-05-11 13:32:12.743254: step 53500, loss = 0.77, accuracy = 0.77 (1134.8 examples/sec; 0.113 sec/batch)
2017-05-11 13:32:23.822787: step 53600, loss = 0.78, accuracy = 0.75 (1174.3 examples/sec; 0.109 sec/batch)
2017-05-11 13:32:35.067218: step 53700, loss = 0.73, accuracy = 0.79 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-11 13:32:46.454626: step 53800, loss = 0.61, accuracy = 0.84 (990.1 examples/sec; 0.129 sec/batch)
2017-05-11 13:32:57.514101: step 53900, loss = 0.61, accuracy = 0.84 (1339.5 examples/sec; 0.096 sec/batch)
2017-05-11 13:33:08.774465: step 54000, loss = 0.60, accuracy = 0.87 (1327.0 examples/sec; 0.096 sec/batch)
2017-05-11 13:33:19.936602: step 54100, loss = 0.63, accuracy = 0.84 (1328.0 examples/sec; 0.096 sec/batch)
2017-05-11 13:33:31.267964: step 54200, loss = 0.73, accuracy = 0.85 (1231.1 examples/sec; 0.104 sec/batch)
2017-05-11 13:33:42.518021: step 54300, loss = 0.69, accuracy = 0.85 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-11 13:33:53.674360: step 54400, loss = 0.72, accuracy = 0.83 (1258.6 examples/sec; 0.102 sec/batch)
2017-05-11 13:34:04.939096: step 54500, loss = 0.57, accuracy = 0.91 (1095.3 examples/sec; 0.117 sec/batch)
2017-05-11 13:34:16.054777: step 54600, loss = 0.75, accuracy = 0.82 (1107.0 examples/sec; 0.116 sec/batch)
2017-05-11 13:34:27.464391: step 54700, loss = 0.64, accuracy = 0.83 (999.5 examples/sec; 0.128 sec/batch)
2017-05-11 13:34:38.911195: step 54800, loss = 0.72, accuracy = 0.80 (1062.1 examples/sec; 0.121 sec/batch)
2017-05-11 13:34:50.110012: step 54900, loss = 0.77, accuracy = 0.82 (1011.6 examples/sec; 0.127 sec/batch)
2017-05-11 13:35:01.475365: step 55000, loss = 0.73, accuracy = 0.84 (1148.6 examples/sec; 0.111 sec/batch)
2017-05-11 13:35:12.805447: step 55100, loss = 0.67, accuracy = 0.84 (1152.2 examples/sec; 0.111 sec/batch)
2017-05-11 13:35:23.531177: step 55200, loss = 0.74, accuracy = 0.80 (1043.6 examples/sec; 0.123 sec/batch)
2017-05-11 13:35:34.538352: step 55300, loss = 0.60, accuracy = 0.84 (1098.2 examples/sec; 0.117 sec/batch)
2017-05-11 13:35:45.892491: step 55400, loss = 0.59, accuracy = 0.88 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-11 13:35:57.079960: step 55500, loss = 0.92, accuracy = 0.76 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-11 13:36:08.415892: step 55600, loss = 0.55, accuracy = 0.89 (838.7 examples/sec; 0.153 sec/batch)
2017-05-11 13:36:19.534468: step 55700, loss = 0.76, accuracy = 0.80 (1183.0 examples/sec; 0.108 sec/batch)
2017-05-11 13:36:30.587123: step 55800, loss = 0.79, accuracy = 0.82 (1144.3 examples/sec; 0.112 sec/batch)
2017-05-11 13:36:41.865400: step 55900, loss = 0.68, accuracy = 0.84 (1066.5 examples/sec; 0.120 sec/batch)
2017-05-11 13:36:53.031054: step 56000, loss = 0.69, accuracy = 0.83 (977.3 examples/sec; 0.131 sec/batch)
2017-05-11 13:37:04.450422: step 56100, loss = 0.92, accuracy = 0.75 (1238.5 examples/sec; 0.103 sec/batch)
2017-05-11 13:37:15.936228: step 56200, loss = 0.67, accuracy = 0.81 (1059.6 examples/sec; 0.121 sec/batch)
2017-05-11 13:37:26.862465: step 56300, loss = 0.69, accuracy = 0.84 (1145.0 examples/sec; 0.112 sec/batch)
2017-05-11 13:37:38.186146: step 56400, loss = 0.71, accuracy = 0.83 (1055.4 examples/sec; 0.121 sec/batch)
2017-05-11 13:37:49.448971: step 56500, loss = 0.76, accuracy = 0.83 (1078.8 examples/sec; 0.119 sec/batch)
2017-05-11 13:38:00.558971: step 56600, loss = 0.60, accuracy = 0.88 (1205.3 examples/sec; 0.106 sec/batch)
2017-05-11 13:38:11.852610: step 56700, loss = 0.50, accuracy = 0.90 (1030.6 examples/sec; 0.124 sec/batch)
2017-05-11 13:38:23.312583: step 56800, loss = 0.68, accuracy = 0.86 (1176.8 examples/sec; 0.109 sec/batch)
2017-05-11 13:38:34.739031: step 56900, loss = 0.78, accuracy = 0.80 (1241.5 examples/sec; 0.103 sec/batch)
2017-05-11 13:38:46.009275: step 57000, loss = 0.73, accuracy = 0.84 (1057.1 examples/sec; 0.121 sec/batch)
2017-05-11 13:38:57.527325: step 57100, loss = 0.68, accuracy = 0.83 (998.5 examples/sec; 0.128 sec/batch)
2017-05-11 13:39:08.670671: step 57200, loss = 0.64, accuracy = 0.84 (1120.6 examples/sec; 0.114 sec/batch)
2017-05-11 13:39:19.536840: step 57300, loss = 0.66, accuracy = 0.87 (985.3 examples/sec; 0.130 sec/batch)
2017-05-11 13:39:30.804542: step 57400, loss = 0.51, accuracy = 0.90 (1098.0 examples/sec; 0.117 sec/batch)
2017-05-11 13:39:42.158933: step 57500, loss = 0.73, accuracy = 0.79 (964.5 examples/sec; 0.133 sec/batch)
2017-05-11 13:39:52.874015: step 57600, loss = 0.62, accuracy = 0.88 (1141.4 examples/sec; 0.112 sec/batch)
2017-05-11 13:40:04.132778: step 57700, loss = 0.59, accuracy = 0.88 (1029.9 examples/sec; 0.124 sec/batch)
2017-05-11 13:40:15.415838: step 57800, loss = 0.79, accuracy = 0.80 (1090.3 examples/sec; 0.117 sec/batch)
2017-05-11 13:40:26.605750: step 57900, loss = 0.50, accuracy = 0.89 (1150.3 examples/sec; 0.111 sec/batch)
2017-05-11 13:40:37.932951: step 58000, loss = 0.63, accuracy = 0.88 (1210.3 examples/sec; 0.106 sec/batch)
2017-05-11 13:40:49.141156: step 58100, loss = 0.71, accuracy = 0.83 (1184.6 examples/sec; 0.108 sec/batch)
2017-05-11 13:41:00.330688: step 58200, loss = 0.65, accuracy = 0.84 (1078.3 examples/sec; 0.119 sec/batch)
2017-05-11 13:41:11.601263: step 58300, loss = 0.70, accuracy = 0.80 (1037.7 examples/sec; 0.123 sec/batch)
2017-05-11 13:41:22.621677: step 58400, loss = 0.51, accuracy = 0.90 (990.1 examples/sec; 0.129 sec/batch)
2017-05-11 13:41:33.774949: step 58500, loss = 0.59, accuracy = 0.88 (933.9 examples/sec; 0.137 sec/batch)
2017-05-11 13:41:44.818698: step 58600, loss = 0.79, accuracy = 0.82 (1065.3 examples/sec; 0.120 sec/batch)
2017-05-11 13:41:56.050024: step 58700, loss = 0.65, accuracy = 0.85 (1069.1 examples/sec; 0.120 sec/batch)
2017-05-11 13:42:07.371673: step 58800, loss = 0.67, accuracy = 0.84 (1051.5 examples/sec; 0.122 sec/batch)
2017-05-11 13:42:18.422075: step 58900, loss = 0.66, accuracy = 0.87 (979.7 examples/sec; 0.131 sec/batch)
2017-05-11 13:42:29.644862: step 59000, loss = 0.50, accuracy = 0.91 (1014.6 examples/sec; 0.126 sec/batch)
2017-05-11 13:42:40.884613: step 59100, loss = 0.65, accuracy = 0.88 (1172.5 examples/sec; 0.109 sec/batch)
2017-05-11 13:42:52.141011: step 59200, loss = 0.65, accuracy = 0.86 (1104.4 examples/sec; 0.116 sec/batch)
2017-05-11 13:43:03.248837: step 59300, loss = 0.58, accuracy = 0.88 (997.6 examples/sec; 0.128 sec/batch)
2017-05-11 13:43:14.504117: step 59400, loss = 0.64, accuracy = 0.87 (915.2 examples/sec; 0.140 sec/batch)
2017-05-11 13:43:25.771496: step 59500, loss = 0.74, accuracy = 0.87 (1120.4 examples/sec; 0.114 sec/batch)
2017-05-11 13:43:37.115022: step 59600, loss = 0.70, accuracy = 0.81 (1001.2 examples/sec; 0.128 sec/batch)
2017-05-11 13:43:48.347317: step 59700, loss = 0.56, accuracy = 0.88 (1057.8 examples/sec; 0.121 sec/batch)
2017-05-11 13:43:59.366692: step 59800, loss = 0.64, accuracy = 0.86 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-11 13:44:10.341412: step 59900, loss = 0.66, accuracy = 0.86 (1193.5 examples/sec; 0.107 sec/batch)
2017-05-11 13:44:21.398152: step 60000, loss = 0.57, accuracy = 0.87 (1239.6 examples/sec; 0.103 sec/batch)
2017-05-11 13:44:32.547925: step 60100, loss = 0.61, accuracy = 0.86 (1137.0 examples/sec; 0.113 sec/batch)
2017-05-11 13:44:43.602013: step 60200, loss = 0.65, accuracy = 0.80 (1014.2 examples/sec; 0.126 sec/batch)
2017-05-11 13:44:54.649224: step 60300, loss = 0.67, accuracy = 0.89 (1061.0 examples/sec; 0.121 sec/batch)
2017-05-11 13:45:05.712906: step 60400, loss = 0.71, accuracy = 0.80 (1040.1 examples/sec; 0.123 sec/batch)
2017-05-11 13:45:16.762465: step 60500, loss = 0.68, accuracy = 0.86 (1202.5 examples/sec; 0.106 sec/batch)
2017-05-11 13:45:28.019364: step 60600, loss = 0.73, accuracy = 0.82 (1237.3 examples/sec; 0.103 sec/batch)
2017-05-11 13:45:39.085965: step 60700, loss = 0.58, accuracy = 0.86 (1058.8 examples/sec; 0.121 sec/batch)
2017-05-11 13:45:49.923833: step 60800, loss = 0.62, accuracy = 0.88 (1233.4 examples/sec; 0.104 sec/batch)
2017-05-11 13:46:01.022606: step 60900, loss = 0.71, accuracy = 0.84 (1188.5 examples/sec; 0.108 sec/batch)
2017-05-11 13:46:12.173797: step 61000, loss = 0.68, accuracy = 0.77 (974.4 examples/sec; 0.131 sec/batch)
2017-05-11 13:46:23.542632: step 61100, loss = 0.64, accuracy = 0.84 (1032.4 examples/sec; 0.124 sec/batch)
2017-05-11 13:46:34.379961: step 61200, loss = 0.63, accuracy = 0.80 (1217.0 examples/sec; 0.105 sec/batch)
2017-05-11 13:46:45.609491: step 61300, loss = 0.67, accuracy = 0.83 (1076.1 examples/sec; 0.119 sec/batch)
2017-05-11 13:46:56.576319: step 61400, loss = 0.60, accuracy = 0.85 (1170.8 examples/sec; 0.109 sec/batch)
2017-05-11 13:47:07.818314: step 61500, loss = 0.68, accuracy = 0.79 (1289.2 examples/sec; 0.099 sec/batch)
2017-05-11 13:47:18.907864: step 61600, loss = 0.80, accuracy = 0.82 (1065.1 examples/sec; 0.120 sec/batch)
2017-05-11 13:47:30.082050: step 61700, loss = 0.52, accuracy = 0.88 (1282.2 examples/sec; 0.100 sec/batch)
2017-05-11 13:47:41.253957: step 61800, loss = 0.80, accuracy = 0.78 (1024.1 examples/sec; 0.125 sec/batch)
2017-05-11 13:47:52.587778: step 61900, loss = 0.72, accuracy = 0.84 (965.7 examples/sec; 0.133 sec/batch)
2017-05-11 13:48:03.838008: step 62000, loss = 0.56, accuracy = 0.86 (884.8 examples/sec; 0.145 sec/batch)
2017-05-11 13:48:14.967764: step 62100, loss = 0.71, accuracy = 0.85 (900.2 examples/sec; 0.142 sec/batch)
2017-05-11 13:48:26.223311: step 62200, loss = 0.55, accuracy = 0.88 (1049.1 examples/sec; 0.122 sec/batch)
2017-05-11 13:48:37.389516: step 62300, loss = 0.55, accuracy = 0.91 (1039.9 examples/sec; 0.123 sec/batch)
2017-05-11 13:48:48.542057: step 62400, loss = 0.70, accuracy = 0.84 (1041.9 examples/sec; 0.123 sec/batch)
2017-05-11 13:48:59.576950: step 62500, loss = 0.59, accuracy = 0.84 (1026.8 examples/sec; 0.125 sec/batch)
2017-05-11 13:49:10.739250: step 62600, loss = 0.66, accuracy = 0.87 (1114.9 examples/sec; 0.115 sec/batch)
2017-05-11 13:49:21.987574: step 62700, loss = 0.75, accuracy = 0.80 (939.9 examples/sec; 0.136 sec/batch)
2017-05-11 13:49:33.274485: step 62800, loss = 0.73, accuracy = 0.81 (985.4 examples/sec; 0.130 sec/batch)
2017-05-11 13:49:44.646373: step 62900, loss = 0.73, accuracy = 0.80 (979.0 examples/sec; 0.131 sec/batch)
2017-05-11 13:49:55.953855: step 63000, loss = 0.55, accuracy = 0.87 (1266.6 examples/sec; 0.101 sec/batch)
2017-05-11 13:50:07.158412: step 63100, loss = 0.62, accuracy = 0.87 (1055.1 examples/sec; 0.121 sec/batch)
2017-05-11 13:50:18.222909: step 63200, loss = 0.78, accuracy = 0.77 (1052.2 examples/sec; 0.122 sec/batch)
2017-05-11 13:50:29.355828: step 63300, loss = 0.65, accuracy = 0.84 (1206.8 examples/sec; 0.106 sec/batch)
2017-05-11 13:50:40.557457: step 63400, loss = 0.70, accuracy = 0.81 (954.9 examples/sec; 0.134 sec/batch)
2017-05-11 13:50:51.746051: step 63500, loss = 0.63, accuracy = 0.87 (1033.8 examples/sec; 0.124 sec/batch)
2017-05-11 13:51:03.090855: step 63600, loss = 0.61, accuracy = 0.83 (1029.1 examples/sec; 0.124 sec/batch)
2017-05-11 13:51:14.306356: step 63700, loss = 0.91, accuracy = 0.76 (1026.0 examples/sec; 0.125 sec/batch)
2017-05-11 13:51:25.649438: step 63800, loss = 0.81, accuracy = 0.82 (1011.0 examples/sec; 0.127 sec/batch)
2017-05-11 13:51:37.051913: step 63900, loss = 0.57, accuracy = 0.91 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-11 13:51:48.191376: step 64000, loss = 0.82, accuracy = 0.80 (1014.9 examples/sec; 0.126 sec/batch)
2017-05-11 13:51:59.542416: step 64100, loss = 0.72, accuracy = 0.80 (1185.2 examples/sec; 0.108 sec/batch)
2017-05-11 13:52:10.839120: step 64200, loss = 0.83, accuracy = 0.80 (1176.9 examples/sec; 0.109 sec/batch)
2017-05-11 13:52:21.768093: step 64300, loss = 0.71, accuracy = 0.80 (1123.0 examples/sec; 0.114 sec/batch)
2017-05-11 13:52:32.827455: step 64400, loss = 0.73, accuracy = 0.82 (1092.8 examples/sec; 0.117 sec/batch)
2017-05-11 13:52:44.235134: step 64500, loss = 0.72, accuracy = 0.83 (993.0 examples/sec; 0.129 sec/batch)
2017-05-11 13:52:55.720658: step 64600, loss = 0.78, accuracy = 0.84 (1150.4 examples/sec; 0.111 sec/batch)
2017-05-11 13:53:06.788946: step 64700, loss = 0.61, accuracy = 0.88 (1161.1 examples/sec; 0.110 sec/batch)
2017-05-11 13:53:17.704438: step 64800, loss = 0.72, accuracy = 0.80 (1220.6 examples/sec; 0.105 sec/batch)
2017-05-11 13:53:28.899834: step 64900, loss = 0.71, accuracy = 0.81 (1099.0 examples/sec; 0.116 sec/batch)
2017-05-11 13:53:39.991207: step 65000, loss = 0.64, accuracy = 0.87 (988.3 examples/sec; 0.130 sec/batch)
2017-05-11 13:53:51.230072: step 65100, loss = 0.53, accuracy = 0.86 (1175.7 examples/sec; 0.109 sec/batch)
2017-05-11 13:54:02.402594: step 65200, loss = 0.79, accuracy = 0.80 (1176.3 examples/sec; 0.109 sec/batch)
2017-05-11 13:54:13.747199: step 65300, loss = 0.70, accuracy = 0.84 (1027.7 examples/sec; 0.125 sec/batch)
2017-05-11 13:54:25.003125: step 65400, loss = 0.60, accuracy = 0.88 (1163.6 examples/sec; 0.110 sec/batch)
2017-05-11 13:54:36.267424: step 65500, loss = 0.56, accuracy = 0.86 (1014.5 examples/sec; 0.126 sec/batch)
2017-05-11 13:54:47.711131: step 65600, loss = 0.65, accuracy = 0.84 (1084.5 examples/sec; 0.118 sec/batch)
2017-05-11 13:54:58.773283: step 65700, loss = 0.61, accuracy = 0.85 (1321.6 examples/sec; 0.097 sec/batch)
2017-05-11 13:55:10.017108: step 65800, loss = 0.63, accuracy = 0.84 (959.2 examples/sec; 0.133 sec/batch)
2017-05-11 13:55:21.138338: step 65900, loss = 0.53, accuracy = 0.90 (1103.3 examples/sec; 0.116 sec/batch)
2017-05-11 13:55:32.418902: step 66000, loss = 0.65, accuracy = 0.84 (1017.1 examples/sec; 0.126 sec/batch)
2017-05-11 13:55:43.780442: step 66100, loss = 0.60, accuracy = 0.88 (1086.6 examples/sec; 0.118 sec/batch)
2017-05-11 13:55:54.895209: step 66200, loss = 0.60, accuracy = 0.86 (988.8 examples/sec; 0.129 sec/batch)
2017-05-11 13:56:05.992735: step 66300, loss = 0.57, accuracy = 0.85 (1091.1 examples/sec; 0.117 sec/batch)
2017-05-11 13:56:17.262470: step 66400, loss = 0.70, accuracy = 0.84 (984.1 examples/sec; 0.130 sec/batch)
2017-05-11 13:56:28.219675: step 66500, loss = 0.59, accuracy = 0.86 (1072.5 examples/sec; 0.119 sec/batch)
2017-05-11 13:56:39.249349: step 66600, loss = 0.63, accuracy = 0.84 (963.6 examples/sec; 0.133 sec/batch)
2017-05-11 13:56:50.208262: step 66700, loss = 0.65, accuracy = 0.84 (1193.1 examples/sec; 0.107 sec/batch)
2017-05-11 13:57:01.383266: step 66800, loss = 0.54, accuracy = 0.88 (1276.6 examples/sec; 0.100 sec/batch)
2017-05-11 13:57:12.447747: step 66900, loss = 0.56, accuracy = 0.91 (1144.3 examples/sec; 0.112 sec/batch)
2017-05-11 13:57:23.716415: step 67000, loss = 0.63, accuracy = 0.88 (1081.3 examples/sec; 0.118 sec/batch)
2017-05-11 13:57:34.802253: step 67100, loss = 0.59, accuracy = 0.88 (1113.5 examples/sec; 0.115 sec/batch)
2017-05-11 13:57:45.982476: step 67200, loss = 0.59, accuracy = 0.86 (988.2 examples/sec; 0.130 sec/batch)
2017-05-11 13:57:57.098323: step 67300, loss = 0.65, accuracy = 0.83 (1092.2 examples/sec; 0.117 sec/batch)
2017-05-11 13:58:08.326952: step 67400, loss = 0.54, accuracy = 0.90 (998.7 examples/sec; 0.128 sec/batch)
2017-05-11 13:58:23.593782: step 67500, loss = 0.78, accuracy = 0.81 (982.8 examples/sec; 0.130 sec/batch)
2017-05-11 13:58:35.646304: step 67600, loss = 0.66, accuracy = 0.84 (1194.3 examples/sec; 0.107 sec/batch)
2017-05-11 13:58:47.450838: step 67700, loss = 0.62, accuracy = 0.84 (1142.1 examples/sec; 0.112 sec/batch)
2017-05-11 13:59:00.147531: step 67800, loss = 0.72, accuracy = 0.83 (1317.2 examples/sec; 0.097 sec/batch)
2017-05-11 13:59:11.745261: step 67900, loss = 0.67, accuracy = 0.82 (1171.5 examples/sec; 0.109 sec/batch)
2017-05-11 13:59:23.797842: step 68000, loss = 0.68, accuracy = 0.84 (1072.3 examples/sec; 0.119 sec/batch)
2017-05-11 13:59:36.358983: step 68100, loss = 0.74, accuracy = 0.81 (1084.2 examples/sec; 0.118 sec/batch)
2017-05-11 13:59:47.543117: step 68200, loss = 0.52, accuracy = 0.88 (1370.7 examples/sec; 0.093 sec/batch)
2017-05-11 13:59:59.925185: step 68300, loss = 0.75, accuracy = 0.79 (1146.1 examples/sec; 0.112 sec/batch)
2017-05-11 14:00:10.453141: step 68400, loss = 0.51, accuracy = 0.91 (1387.8 examples/sec; 0.092 sec/batch)
2017-05-11 14:00:21.655767: step 68500, loss = 0.68, accuracy = 0.81 (963.0 examples/sec; 0.133 sec/batch)
2017-05-11 14:00:33.657085: step 68600, loss = 0.65, accuracy = 0.80 (1357.3 examples/sec; 0.094 sec/batch)
2017-05-11 14:00:44.260251: step 68700, loss = 0.70, accuracy = 0.83 (1382.5 examples/sec; 0.093 sec/batch)
2017-05-11 14:00:55.218559: step 68800, loss = 0.67, accuracy = 0.87 (1292.3 examples/sec; 0.099 sec/batch)
2017-05-11 14:01:06.696412: step 68900, loss = 0.55, accuracy = 0.89 (1187.0 examples/sec; 0.108 sec/batch)
2017-05-11 14:01:18.308773: step 69000, loss = 0.78, accuracy = 0.80 (1043.7 examples/sec; 0.123 sec/batch)
2017-05-11 14:01:28.975667: step 69100, loss = 0.59, accuracy = 0.87 (1270.0 examples/sec; 0.101 sec/batch)
2017-05-11 14:01:40.680369: step 69200, loss = 0.69, accuracy = 0.84 (1163.6 examples/sec; 0.110 sec/batch)
2017-05-11 14:01:52.067065: step 69300, loss = 0.59, accuracy = 0.87 (1314.2 examples/sec; 0.097 sec/batch)
2017-05-11 14:02:03.482335: step 69400, loss = 0.68, accuracy = 0.83 (1415.1 examples/sec; 0.090 sec/batch)
2017-05-11 14:02:15.350625: step 69500, loss = 0.73, accuracy = 0.85 (1331.6 examples/sec; 0.096 sec/batch)
2017-05-11 14:02:27.220221: step 69600, loss = 0.67, accuracy = 0.84 (1023.3 examples/sec; 0.125 sec/batch)
2017-05-11 14:02:39.879463: step 69700, loss = 0.70, accuracy = 0.80 (1026.4 examples/sec; 0.125 sec/batch)
2017-05-11 14:02:51.564917: step 69800, loss = 0.68, accuracy = 0.84 (1086.9 examples/sec; 0.118 sec/batch)
2017-05-11 14:03:04.222504: step 69900, loss = 0.53, accuracy = 0.88 (1293.3 examples/sec; 0.099 sec/batch)
2017-05-11 14:03:16.824442: step 70000, loss = 0.63, accuracy = 0.83 (1204.4 examples/sec; 0.106 sec/batch)

 Average train accuracy: 0.802837
 Max train accuracy: 0.914062
