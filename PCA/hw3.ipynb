{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## CS498 Applied Machine Learning Homework3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Huamin Zhang   &&   Rongzi Wang\n",
    "\n",
    "Monday, Feburary 20, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.10\n",
    "CIFAR-10 is a dataset of 32x32 images in 10 categories, collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. It is often used to evaluate machine learning algorithms. You can download this dataset from https://www.cs.toronto.edu/∼kriz/cifar.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### (a)\n",
    "For each category, compute the mean image and the first 20 principal components. Plot the error resulting from representing the images of each \n",
    "category using the first 20 principal components against the category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read data of all training data\n",
    "wd = \"C:\\\\Users\\\\98302\\\\Desktop\\\\cifar-10-batches-py\\\\data_batch_\"\n",
    "batch_file = [\"1\",\"2\", \"3\", \"4\", \"5\"]\n",
    "data = []\n",
    "label = []\n",
    "for file in batch_file:\n",
    "    input_file = wd + file\n",
    "    fo = open(input_file, 'rb')\n",
    "    dict = pickle.load(fo)\n",
    "    fo.close()\n",
    "    data.append(dict['data'])\n",
    "    label.append(dict['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1553.1472639290423,\n",
       " 1955.6153325333514,\n",
       " 1497.7214125909268,\n",
       " 1714.0358977724477,\n",
       " 1422.0067679182389,\n",
       " 1749.0792671009447,\n",
       " 1576.0715795335107,\n",
       " 1811.8674420936179,\n",
       " 1501.9119009409098,\n",
       " 1972.6160563358567]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############4.10(a)##################################################\n",
    "class_data=[]\n",
    "class_pca=[]\n",
    "class_mean_image=[]\n",
    "error = []\n",
    "for i in range(0,10):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    x=np.array(x,dtype = np.uint8)\n",
    "    new_dict = {'data':x,'labels':y}\n",
    "    \n",
    "    #classfier\n",
    "    for j in range(0,5):\n",
    "        for k in range(0,10000):\n",
    "            if label[j][k] == i:\n",
    "                new_dict['labels'].append(label[j][k])\n",
    "                new_dict['data'] = np.concatenate((new_dict['data'],data[j][k]))\n",
    "    new_dict['data'] = new_dict['data'].reshape(new_dict['data'].shape[0]/3072,3072)\n",
    "    class_data.append(new_dict)\n",
    "    \n",
    "    #do pca\n",
    "    pca = PCA(copy=True,n_components=20) #pca para\n",
    "    new_data = pca.fit_transform(class_data[i]['data']) #transform 10000*3072 to 10000*20\n",
    "    #mean image\n",
    "    class_mean_image.append(pca.mean_) \n",
    "    class_pca.append(new_data)\n",
    "    \n",
    "    #Constructing a low-dimensional representation\n",
    "    represent_data = pca.inverse_transform(new_data)\n",
    "\n",
    "    #calculate error\n",
    "    total_dis = 0\n",
    "    for image in range(5000):\n",
    "        dist = np.sqrt(np.sum(np.square(represent_data[image] - class_data[i]['data'][image])))\n",
    "        total_dis = total_dis + dist\n",
    "\n",
    "    ave_err = total_dis/5000\n",
    "    error.append(ave_err)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the question, we use the formula $\\frac{1}{N} \\sum \\sqrt{\\sum_{i=1}^{3072} (image[m]_i -image[n]_i)^2}$ to calculate the average error between two classes. So the error resulting from representing the images of each category using the first 20 principal components against the category is (1553.15382446685, 1955.6179886516406,1497.7200032304538,1714.0308064609096, 1422.0085695761902,1749.0835367137101, 1576.0729220159319,1811.8635062055494, 1501.9083558302561,1972.6160816894055). The barplot of the error is showed as following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](barplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### (b)\n",
    "Compute the distances between mean images for each pair of classes. Use principal coordinate analysis to make a 2D map of the means of each\n",
    "categories. For this exercise, compute distances by thinking of the images as vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.11872731e+06,   1.43736291e+06,   5.23337909e+05,\n",
       "         1.50913516e+05,   1.39065005e+05,  -3.15542556e-10,\n",
       "         1.22613194e+04,   4.19451647e+04,   2.88933057e+04,\n",
       "         3.18558605e+04])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############4.10(b)##################################################\n",
    "\n",
    "#create distance matrix\n",
    "D = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        dist = np.sum(np.square(class_mean_image[i] - class_mean_image[j]))\n",
    "        D.append(dist)\n",
    "D = np.array(D).reshape(10,10)\n",
    "#Form A,W and get the eigenvectors andeigenvalues of W\n",
    "I = np.identity(10)\n",
    "A = I - 0.1 * np.array([1]*100).reshape(10,10)\n",
    "W = -0.5 * np.dot(np.dot(A,D),np.transpose(A))\n",
    "eigval, eigvec = np.linalg.eig(W)\n",
    "eigval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1240.98660652,  -642.90599027],\n",
       "       [  124.59424171,   502.18646509],\n",
       "       [ -232.62552266,  -151.03722973],\n",
       "       [ -551.42585341,  -171.53378739],\n",
       "       [ -770.97022443,   -68.40681696],\n",
       "       [ -623.48772513,  -371.41540556],\n",
       "       [ -983.32529277,   262.41028018],\n",
       "       [ -260.2478262 ,   -46.83221196],\n",
       "       [ 1218.75239766,   -23.36626178],\n",
       "       [  837.7491987 ,   710.90095838]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the top left r × r block , r =2\n",
    "sort1 = eigval.argsort()[-1]\n",
    "sort2 = eigval.argsort()[-2]\n",
    "v1 = eigval[sort1]\n",
    "v2 = eigval[sort2]\n",
    "vec1 = eigvec[:,sort1]\n",
    "vec2 = eigvec[:,sort2]\n",
    "diag = np.array([np.sqrt(v1),0,0,np.sqrt(v2)]).reshape(2,2)\n",
    "eigvec_r = np.concatenate((vec1,vec2)).reshape(2,10)\n",
    "V_T = np.dot(diag,eigvec_r)\n",
    "V = np.transpose(V_T)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So the V matrix is showed as above. The 2D map of the means of each categories is showed as following and we can find animals are closest to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### (c)\n",
    "Here is another measure of the similarity of two classes. For class A and class B, define $E(A \\rightarrow B)$ to be the average error obtained by representing all the images of class A using the mean of class A and the first 20 principal components of class B. Now define the similarity between classes to be $(1/2)(E(A \\rightarrow B)+E(B \\rightarrow A))$. Use principal coordinate analysis to make a 2D map of the classes. Compare this map to the map in the previous exercise are they different? why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#############4.10(c)##################################################\n",
    "\n",
    "#create E matrix E(i->j) = E_ij\n",
    "E = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        pca_i = PCA(copy=True,n_components=20) #pca para\n",
    "        data_i = pca_i.fit_transform(class_data[i]['data']) #transform 10000*3072 to 10000*20\n",
    "        pca_j = PCA(copy=True,n_components=20) #pca para\n",
    "        data_j = pca_j.fit_transform(class_data[j]['data']) #transform 10000*3072 to 10000*20\n",
    "        pca_i.components_ = pca_j.components_\n",
    "        represent_data_i = pca_i.inverse_transform(data_i)\n",
    "        total_dis = 0\n",
    "        for image in range(5000):\n",
    "            dist = np.sqrt(np.sum(np.square(represent_data_i[image] - class_data[i]['data'][image])))\n",
    "            total_dis = total_dis + dist\n",
    "        ave_err = total_dis/5000\n",
    "        E.append(ave_err)\n",
    "E = np.array(E).reshape(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.45452415e+03,  -4.85232501e-13,   2.32606753e+03,\n",
       "         2.12965284e+03,   1.32628504e+03,   1.76288247e+03,\n",
       "         1.72046181e+03,   1.66284661e+03,   1.51763770e+03,\n",
       "         1.49982041e+03])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D2 = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        similarity = 0.5 * (E[i][j] + E[j][i])\n",
    "        if i == j:\n",
    "            similarity = 0\n",
    "        D2.append(similarity)\n",
    "D2 = np.array(D2).reshape(10,10)\n",
    "#Form A,W and get the eigenvectors andeigenvalues of W\n",
    "I2 = np.identity(10)\n",
    "A2 = I2 - 0.1 * np.array([1]*100).reshape(10,10)\n",
    "W2 = -0.5 * np.dot(np.dot(A2,D2),np.transpose(A2))\n",
    "eigval2, eigvec2 = np.linalg.eig(W2)\n",
    "eigval2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-34.59375249,   1.91904878],\n",
       "       [ 14.82201305,   3.76305174],\n",
       "       [ 17.99645891, -12.64307444],\n",
       "       [ 18.12126984, -34.2490615 ],\n",
       "       [  8.84147255,  10.69525577],\n",
       "       [ 11.15202304,  23.24730935],\n",
       "       [ 14.24731381,  10.25328994],\n",
       "       [ 10.50577661,   7.03159535],\n",
       "       [-31.92532567,   2.6006685 ],\n",
       "       [-29.16724964, -12.61808349]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the top left r × r block , r =2\n",
    "sort_c1 = eigval2.argsort()[-1]\n",
    "sort_c2 = eigval2.argsort()[-2]\n",
    "v_c1 = eigval2[sort_c1]\n",
    "v_c2 = eigval2[sort_c2]\n",
    "vec_c1 = eigvec2[:,sort_c1]\n",
    "vec_c2 = eigvec2[:,sort_c2]\n",
    "diag2 = np.array([np.sqrt(v_c1),0,0,np.sqrt(v_c2)]).reshape(2,2)\n",
    "eigvec_r2 = np.concatenate((vec_c1,vec_c2)).reshape(2,10)\n",
    "#compute V\n",
    "V_T2 = np.dot(diag2,eigvec_r2)\n",
    "V2 = np.transpose(V_T2)\n",
    "V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The PCA model from sklearn uses mean and principal components. The mean and pcs are typically set during the call of the function fit which calculates mean and pcs based on a set of data.  There are also function transform and inverse transform that take data and convert it to their representation in terms pcs or the opposite direction. The transform functions only behave based on the mean and pcs. The mean and pcs can be set manually. Therefore we can do part c by manually setting the mean and components.\n",
    "\n",
    "Here we make diagonals of distance matrix to be 0.\n",
    "\n",
    "So the V matrix for another measure of the similarity of two classes is showed as above. The 2D map of the means of each categories is showed as following.\n",
    "\n",
    "By comparing the scatter plots we obtain from part b with part c, we observe that they are different. For part b, we start by projecting the distance between each classes into an L2 distance in a larger number of dimensions, and the newly constructed 2D map captures the largest amount of variation from the n-1 dimension space. We observe that classes with different animals are close to each other. However, in part c, the average error is obtained by taking the difference between a mean vector and 20 principal components.The mean vector is not correspond to 20 principal components when we construct a low-dimensional representation. Such difference may not reflect the largest variance between classes, and the constructed 2D map might not very well reflect the similarity/difference between all classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](p3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-31.90196183,   0.32197992],\n",
       "       [ 12.69776374,   3.46280928],\n",
       "       [ 16.7461959 , -11.96139016],\n",
       "       [ 16.2738602 , -26.98157014],\n",
       "       [  8.24179315,   9.95928546],\n",
       "       [  9.82406533,  17.53399166],\n",
       "       [ 13.03246106,   9.19088388],\n",
       "       [  9.22275975,   5.80136343],\n",
       "       [-29.276331  ,   0.92405535],\n",
       "       [-24.86060629,  -8.25140868]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############extra##################################################\n",
    "D3 = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        similarity = 0.5 * (E[i][j] + E[j][i])\n",
    "        D3.append(similarity)\n",
    "D3 = np.array(D3).reshape(10,10)\n",
    "#Form A,W and get the eigenvectors andeigenvalues of W\n",
    "I3 = np.identity(10)\n",
    "A3 = I3 - 0.1 * np.array([1]*100).reshape(10,10)\n",
    "W3 = -0.5 * np.dot(np.dot(A3,D3),np.transpose(A3))\n",
    "eigval3, eigvec3 = np.linalg.eig(W3)\n",
    "#the top left r × r block , r =2\n",
    "sort_d1 = eigval3.argsort()[-1]\n",
    "sort_d2 = eigval3.argsort()[-2]\n",
    "v_d1 = eigval3[sort_d1]\n",
    "v_d2 = eigval3[sort_d2]\n",
    "vec_d1 = eigvec3[:,sort_d1]\n",
    "vec_d2 = eigvec3[:,sort_d2]\n",
    "diag3 = np.array([np.sqrt(v_d1),0,0,np.sqrt(v_d2)]).reshape(2,2)\n",
    "eigvec_r2 = np.concatenate((vec_d1,vec_d2)).reshape(2,10)\n",
    "#compute V\n",
    "V_T3 = np.dot(diag3,eigvec_r2)\n",
    "V3 = np.transpose(V_T3)\n",
    "V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try another diagonals possible(non-zero) and the result is following. We find the result are almost the same. The result may be that the centering matrix is a symmetric and idempotent matrix, which when multiplied with \n",
    "a vector has the same effect as subtracting the mean of the components of the vector from every component so it dose not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](p4.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
